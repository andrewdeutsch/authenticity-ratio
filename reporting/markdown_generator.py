"""
Markdown report generator for AR tool
Creates markdown reports for easy sharing and documentation
"""

from typing import Dict, Any, List, Optional
import math
import logging
from datetime import datetime
import os
import re
import json
from statistics import mean

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import numpy as np

logger = logging.getLogger(__name__)

try:
    # Optional: use scikit-learn TF-IDF if installed for better scoring
    from sklearn.feature_extraction.text import TfidfVectorizer
    _HAVE_SKLEARN = True
except Exception:
    TfidfVectorizer = None
    _HAVE_SKLEARN = False


def add_llm_provenance(text: str, model: str) -> str:
    """
    Add consistent provenance labeling to LLM-generated text.

    This helper ensures all LLM-generated descriptions and summaries across
    report generators include a standardized provenance label.

    Args:
        text: The LLM-generated text
        model: The model name (e.g., 'gpt-3.5-turbo', 'gpt-4')

    Returns:
        Text with appended provenance label in format: "(Generated by <model>)"

    Example:
        >>> add_llm_provenance("This is a summary.", "gpt-3.5-turbo")
        "This is a summary. (Generated by gpt-3.5-turbo)"
    """
    if not text or not text.strip():
        return text

    text = text.strip()

    # Check if provenance already exists (avoid double-labeling)
    if text.endswith(')') and '(Generated by' in text[-50:]:
        return text

    # Append standardized provenance label
    return f"{text} (Generated by {model})"


def _summarize_text(text: str, max_lines: int = 2, max_chars: int = 240, meta_desc: str = None) -> str:
    """Produce a short summary for text.

    Strategy:
      - Prefer meta/OG description when available and sufficiently long.
      - Clean boilerplate/UI fragments.
      - Extract 1-2 representative sentences using TF-IDF/TextRank-style scoring.
      - Enforce max_chars safely.
    """
    if not text:
        return ''

    # 1) Prefer meta description if provided
    if meta_desc:
        md = meta_desc.strip()
        if len(md) >= 30:
            return _trim_summary(md, max_chars)

    # 2) Clean input text
    clean = _clean_text(text)
    if not clean:
        return ''
    if len(clean) <= max_chars:
        return clean

    # 3) Split into candidate sentences
    sentences = _split_sentences(clean)
    if not sentences:
        return _trim_summary(clean, max_chars)
    if len(sentences) <= max_lines:
        joined = ' '.join(sentences)
        return _trim_summary(joined, max_chars)

    # 4) Score and select best sentences
    chosen = _score_and_select(sentences, max_lines=max_lines)

    # Preserve original order
    ordered = [s for s in sentences if s in chosen]
    out = ' '.join(ordered[:max_lines])
    return _trim_summary(out, max_chars)


def _clean_text(text: str) -> str:
    s = html_unescape(text)
    s = re.sub(r"\s+", ' ', s).strip()
    # remove common UI/boilerplate fragments
    stop_phrases = [
        'Learn more', 'Read more', 'Close', 'Cookie settings', 'Subscribe',
        'Sign in', 'Log in', 'What can I help with', 'Show more'
    ]
    for p in stop_phrases:
        s = s.replace(p, '')
    # Drop very short fragments and multilingual short UI snippets often scraped from interactive widgets
    # e.g., small phrases or single-word bullets
    s = re.sub(r"(?:\b[A-Za-z]{1,3}\b\s*){1,4}", '', s)
    # Remove leftover sequences of non-word punctuation often found in nav bars
    s = re.sub(r"[\-_=]{2,}", ' ', s)
    s = s.strip(' \t\n\r\u200b\ufeff')
    return s


def html_unescape(text: str) -> str:
    try:
        import html
        return html.unescape(text)
    except Exception:
        return text


def _split_sentences(text: str) -> List[str]:
    # Simple but pragmatic sentence splitter
    parts = re.split(r'(?<=[\.\!\?])\s+', text)
    parts = [p.strip() for p in parts if len(p.strip()) > 10]
    return parts


def clean_text_for_llm(meta: Dict[str, Any]) -> str:
    """Build a reasonable text blob for LLM summarization from metadata dict."""
    parts = []
    if not isinstance(meta, dict):
        return ''
    for key in ('title', 'name', 'headline'):
        v = meta.get(key)
        if v:
            parts.append(str(v))
    for key in ('description', 'snippet', 'summary', 'body'):
        v = meta.get(key)
        if v:
            parts.append(str(v))
    # fallback to URL if nothing else
    url = meta.get('source_url') or meta.get('url')
    if url:
        parts.append(url)
    return '\n\n'.join(parts)


def _score_and_select(sentences: List[str], max_lines: int = 2) -> List[str]:
    # Use sklearn TF-IDF if available, otherwise a simple heuristic
    scores = []
    if _HAVE_SKLEARN and len(sentences) > 0:
        try:
            vec = TfidfVectorizer(stop_words='english')
            X = vec.fit_transform(sentences)
            scores = X.sum(axis=1).A1.tolist()
        except Exception:
            scores = [_simple_score(s) for s in sentences]
    else:
        scores = [_simple_score(s) for s in sentences]

    ranked = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)
    selected = [sentences[i] for i in ranked[: max_lines * 3]]
    # dedupe and keep up to max_lines
    out = []
    seen = set()
    for s in selected:
        key = s.lower()
        if key in seen:
            continue
        seen.add(key)
        out.append(s)
        if len(out) >= max_lines:
            break
    return out


def _simple_score(s: str) -> float:
    words = re.findall(r"\w+", s.lower())
    if not words:
        return 0.0
    freq = {}
    for w in words:
        freq[w] = freq.get(w, 0) + 1
    score = sum(1.0 / math.sqrt(v) for v in freq.values())
    score *= min(1.5, len(words) / 10)
    return score


def _trim_summary(text: str, max_chars: int) -> str:
    t = text.strip()
    if len(t) <= max_chars:
        return t
    cut = t[: max_chars]
    # try to end at sentence boundary
    idx = cut.rfind('.')
    if idx > int(max_chars * 0.5):
        return cut[: idx + 1].strip() + '‚Ä¶'
    # else cut on word boundary
    safe = cut.rsplit(' ', 1)[0].rstrip('.,;:')
    return safe + '‚Ä¶'


def _llm_summarize(text: str, *, model: str = 'gpt-3.5-turbo', max_words: int = 120) -> Optional[str]:
    """Optional abstractive summarizer using internal ChatClient wrapper.

    Returns None if LLM client is not available/configured.
    """
    # When running under pytest, avoid calling external LLMs to keep tests
    # deterministic and offline. Pytest sets the PYTEST_CURRENT_TEST env var
    # during test execution; if present, return None immediately.
    try:
        import os as _os
        if 'PYTEST_CURRENT_TEST' in _os.environ:
            return None
    except Exception:
        pass

    try:
        # Lazy import to avoid hard dependency
        from scoring.llm_client import ChatClient
    except Exception:
        return None
    prompt = (
        f"Write a concise {max_words}-word human-readable summary (1-2 lines) of the following content.\n\nContent:\n{text}\n\nSummary:")
    client = ChatClient()
    try:
        resp = client.chat(model=model, messages=[{"role": "user", "content": prompt}], max_tokens=300)
        return resp.get('text') or resp.get('content') or None
    except Exception:
        return None

class MarkdownReportGenerator:
    """Generates Markdown reports for AR analysis"""
    
    def generate_report(self, report_data: Dict[str, Any], output_path: str) -> str:
        """
        Generate Markdown report from report data
        
        Args:
            report_data: Dictionary containing report data
            output_path: Path to save the Markdown file
            
        Returns:
            Path to generated Markdown file
        """
        logger.info(f"Generating Markdown report: {output_path}")
        
        markdown_content = self._build_markdown_content(report_data)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        logger.info(f"Markdown report generated successfully: {output_path}")
        return output_path
    
    def _build_markdown_content(self, report_data: Dict[str, Any]) -> str:
        """Build the complete markdown content"""
        content = []

        # Title
        content.append(self._create_header(report_data))

        # Executive summary FIRST (Trust Stack focused - no AR metrics)
        content.append(self._create_executive_summary(report_data))

        # Metadata SECOND (collapsible reference details)
        content.append(self._create_metadata(report_data))

        # Dimension breakdown
        content.append(self._create_dimension_breakdown(report_data))

        # Classification analysis
        content.append(self._create_classification_analysis(report_data))

        # Recommendations
        content.append(self._create_recommendations(report_data))

        # Appendix (per-item diagnostics)
        content.append(self._create_appendix(report_data))

        # ============ AUTHENTICITY RATIO INTEGRATION (DISABLED) ============
        # The AR Analysis section is preserved but commented out during Trust Stack rebrand.
        # To re-enable AR metrics in reports:
        # 1. Uncomment the line below to add AR Analysis section
        # 2. Update header from "Trust Stack Content Analysis" to "Authenticity Ratio‚Ñ¢ Report" (line 293)
        # 3. Update footer to reference AR instead of Trust Stack (line 786)
        # 4. Update Recommendations to use ar_pct instead of dimension scores (see git history for original logic)
        #
        # content.append(self._create_ar_analysis(report_data))
        # ============ END AR INTEGRATION ============

        # Footer
        content.append(self._create_footer(report_data))

        return "\n\n".join(content)
    
    def _create_header(self, report_data: Dict[str, Any]) -> str:
        """Create report header"""
        brand_id = report_data.get('brand_id', 'Unknown Brand')
        return f"# Trust Stack Content Analysis\n\n## Brand: {brand_id}"
    
    def _create_metadata(self, report_data: Dict[str, Any]) -> str:
        """Create metadata section (collapsible)"""
        run_id = report_data.get('run_id', 'Unknown')
        generated_at = report_data.get('generated_at', datetime.now().isoformat())
        rubric_version = report_data.get('rubric_version', 'v1.0')
        total_items = report_data.get('authenticity_ratio', {}).get('total_items', 0)

        return f"""---

<details>
<summary><b>üìã Report Metadata</b> (click to expand)</summary>

| Field | Value |
|-------|-------|
| **Run ID** | `{run_id}` |
| **Generated** | {generated_at} |
| **Items Analyzed** | {total_items:,} |
| **Data Sources** | {', '.join(report_data.get('sources', [])) if report_data.get('sources') else 'Unknown'} |
| **Rubric Version** | {rubric_version} |
| **Methodology** | See [AR_METHODOLOGY.md](../docs/AR_METHODOLOGY.md) |

</details>

---"""
    
    def _create_executive_summary(self, report_data: Dict[str, Any]) -> str:
        """Create executive summary section - Trust Stack focused"""
        ar_data = report_data.get('authenticity_ratio', {})
        total_items = ar_data.get('total_items', 0)
        dimension_breakdown = report_data.get('dimension_breakdown', {})

        # Build Trust Stack ratings table
        trust_stack_lines = ["### Trust Stack Ratings"]
        trust_stack_lines.append("")  # Blank line after header
        trust_stack_lines.append("| Dimension | Average Score | Status | Key Insight |")
        trust_stack_lines.append("|-----------|---------------|--------|-------------|")

        dimension_interpretations = {
            'provenance': 'Content traceability and source verification',
            'verification': 'Alignment with authoritative brand data',
            'transparency': 'Clarity of ownership and disclosure',
            'coherence': 'Consistency with brand messaging',
            'resonance': 'Authentic audience engagement'
        }

        weakest_dim = None
        weakest_score = 1.0
        strongest_dim = None
        strongest_score = 0.0

        for dim in ['provenance', 'verification', 'transparency', 'coherence', 'resonance']:
            stats = dimension_breakdown.get(dim, {})
            avg = stats.get('average', 0.0)

            # Track weakest and strongest
            if avg < weakest_score:
                weakest_score = avg
                weakest_dim = dim
            if avg > strongest_score:
                strongest_score = avg
                strongest_dim = dim

            # Status indicator (consistent with Dimension Performance section)
            if avg >= 0.80:
                status = "üü¢ Excellent"
            elif avg >= 0.60:
                status = "üü° Good"
            elif avg >= 0.40:
                status = "üü† Moderate"
            else:
                status = "üî¥ Poor"

            insight = dimension_interpretations.get(dim, '')
            trust_stack_lines.append(f"| **{dim.title()}** | {avg:.3f} | {status} | {insight} |")

        trust_stack_table = "\n".join(trust_stack_lines)

        # Build interpretation paragraph focused on dimensions
        interp = (
            f"Analysis of {total_items:,} brand-related content items reveals trust patterns across five dimensions. "
            f"**{strongest_dim.title()}** is the strongest dimension ({strongest_score:.3f}), "
            f"while **{weakest_dim.title()}** ({weakest_score:.3f}) requires attention. "
        )

        # Add specific guidance based on weakest dimension
        if weakest_dim == 'verification':
            interp += "The low verification scores suggest content lacks authoritative identifiers such as verified domains or official brand handles."
        elif weakest_dim == 'transparency':
            interp += "The low transparency scores indicate missing disclosure tags or ambiguous authorship."
        elif weakest_dim == 'provenance':
            interp += "The low provenance scores suggest content lacks clear source attribution or traceability."
        elif weakest_dim == 'coherence':
            interp += "The low coherence scores indicate inconsistent messaging that may signal unofficial content."
        elif weakest_dim == 'resonance':
            interp += "The low resonance scores suggest engagement patterns don't align with authentic brand communities."

        # Build images (heatmap, trendline, channel breakdown) and include them if created
        visuals_md = []
        # Heatmap from dimension breakdown
        heatmap_path = self._create_dimension_heatmap(report_data)
        if heatmap_path:
            visuals_md.append(f"![5D Trust Heatmap]({heatmap_path})")

        # Channel breakdown (if provided)
        channel_path = self._create_channel_breakdown(report_data)
        if channel_path:
            visuals_md.append(f"![Channel Breakdown]({channel_path})")

        # Content-type breakdown visualization
        ctype_path = self._create_content_type_breakdown(report_data)
        if ctype_path:
            visuals_md.append(f"![Content Type Breakdown]({ctype_path})")

        visuals_block = "\n\n".join(visuals_md) if visuals_md else ""

        # Include a short list of example items (up to 3) for the executive.
        # Prefer the detailed per-item diagnostics from the appendix when available
        # so we can show title, description, short analysis, and link.
        examples_md = ''
        appendix = report_data.get('appendix') or []
        items = report_data.get('items', [])
        max_examples = 3

        def _get_meta_from_item(it):
            meta = it.get('meta') or {}
            # meta may be a JSON string in some cases
            if isinstance(meta, str):
                try:
                    import json as _json
                    meta = _json.loads(meta) if meta else {}
                except Exception:
                    meta = {}
            return meta if isinstance(meta, dict) else {}

        # Choose candidate pool: prefer appendix (richer), fallback to items
        pool = appendix if appendix else items
        if pool:
            # Build a mapping of sources in preferred order
            preferred_sources = report_data.get('sources') or sorted({it.get('source') for it in pool if it.get('source')})

            selected = []
            # Try to pick at least one example per source (up to max_examples)
            for src in preferred_sources:
                if len(selected) >= max_examples:
                    break
                for it in pool:
                    if it.get('source') == src and it not in selected:
                        selected.append(it)
                        break

            # Fill remaining slots with top-scoring items (highest final_score)
            if len(selected) < max_examples:
                remaining = [it for it in pool if it not in selected]
                try:
                    remaining.sort(key=lambda x: float(x.get('final_score') or 0.0), reverse=True)
                except Exception:
                    pass
                for it in remaining:
                    if len(selected) >= max_examples:
                        break
                    selected.append(it)

            example_lines = []
            for ex in selected:
                meta = _get_meta_from_item(ex)
                # Robust title fallback: check item first, then meta fields
                title = (
                    ex.get('title') or  # Check top-level title field first
                    meta.get('title') or
                    meta.get('name') or
                    meta.get('headline') or
                    meta.get('og:title') or
                    (meta.get('source_url') if meta.get('source_url') and isinstance(meta.get('source_url'), str) else None)
                )
                if not title:
                    # Try to derive from body/snippet
                    body_text = (meta.get('body') or meta.get('description') or meta.get('snippet') or ex.get('body') or '')
                    if body_text:
                        title = ' '.join(body_text.strip().split()[:7]) + ('...' if len(body_text.split()) > 7 else '')
                    else:
                        title = ex.get('content_id')
                # YouTube-specific fallbacks: prefer channel/title or construct from id
                if (not title or title == ex.get('content_id')):
                    try:
                        url = meta.get('source_url') or meta.get('url') or ''
                        if url and ('youtube.com' in url or 'youtu.be' in url):
                            ch = meta.get('channel_title') or meta.get('publisher') or meta.get('author')
                            vid = None
                            m = re.search(r'(?:v=|youtu\.be/)([A-Za-z0-9_\-]{6,})', url)
                            if m:
                                vid = m.group(1)
                            else:
                                cid = ex.get('content_id') or ex.get('id') or ''
                                mm = re.search(r'youtube_video_(?P<id>.+)', str(cid))
                                if mm:
                                    vid = mm.group('id')
                            if ch:
                                title = f"YouTube: {ch}"
                            elif vid:
                                title = f"YouTube video {vid}"
                    except Exception:
                        pass
                score = float(ex.get('final_score', 0.0) or 0.0)
                label = (ex.get('label') or '').title()
                # description/snippet
                raw_desc = meta.get('description') or meta.get('snippet') or meta.get('summary') or ''
                # Optionally use LLM for executive examples (small curated set)
                use_llm = bool(report_data.get('use_llm_for_examples') or report_data.get('use_llm_for_descriptions'))
                desc = None
                if use_llm:
                    # prefer LLM abstractive summary for the executive example
                    try:
                        desc_llm = _llm_summarize(raw_desc or clean_text_for_llm(meta), model=report_data.get('llm_model', 'gpt-3.5-turbo'), max_words=120)
                        if desc_llm:
                            # Append an explicit provenance label for clarity
                            desc = add_llm_provenance(desc_llm, report_data.get('llm_model', 'gpt-3.5-turbo'))
                    except Exception:
                        desc = None
                if not desc:
                    # fallback to extractive summarizer
                    # Prefer body when snippet is thin/noisy
                    body_text = meta.get('body') or ex.get('body') or ''
                    desc = _summarize_text(raw_desc or body_text, max_lines=2, max_chars=240)

                # Parse dimension scores for trust assessment
                dims = ex.get('dimension_scores') or {}
                dims_parsed = {}
                if isinstance(dims, dict):
                    for k, v in dims.items():
                        try:
                            dims_parsed[k] = float(v)
                        except Exception:
                            pass

                # Create trust assessment explanation based on dimension scores
                trust_assessment = ""
                if dims_parsed:
                    # Find weakest and strongest dimensions for this item
                    sorted_dims = sorted(dims_parsed.items(), key=lambda x: x[1])
                    weakest_item_dims = sorted_dims[:2] if len(sorted_dims) >= 2 else sorted_dims
                    strongest_item_dims = sorted(dims_parsed.items(), key=lambda x: x[1], reverse=True)[:1]

                    # Determine trust level based on score
                    if score >= 0.70:
                        trust_level = "High Trust"
                    elif score >= 0.50:
                        trust_level = "Moderate Trust"
                    else:
                        trust_level = "Low Trust"

                    # Build explanation
                    trust_assessment = f"**Trust Assessment**: {trust_level} rating ({score:.2f}) "

                    if weakest_item_dims:
                        weak_dims_str = ', '.join([f"{k.title()} ({v:.2f})" for k, v in weakest_item_dims])
                        trust_assessment += f"with weaker signals in {weak_dims_str}. "

                    if strongest_item_dims:
                        strong_dims_str = ', '.join([f"{k.title()} ({v:.2f})" for k, v in strongest_item_dims])
                        trust_assessment += f"Strongest dimension: {strong_dims_str}."
                else:
                    trust_assessment = f"**Trust Assessment**: Score: {score:.2f}"

                # Get URL - only show real URLs, not platform IDs
                url = meta.get('source_url') or meta.get('url') or meta.get('source_link') or ''
                # Validate URL - must start with http:// or https://
                if url and not (url.startswith('http://') or url.startswith('https://')):
                    # Check if it's a YouTube ID or other platform ID (no dots, no slashes)
                    if '.' not in url and '/' not in url:
                        url = ''  # Clear platform IDs like "z1fgz5db-I4"

                # Fallback: if no URL but we have a YouTube id or platform id, construct a YouTube URL
                if not url:
                    try:
                        # video id may be in meta or top-level fields
                        vid = (meta.get('video_id') or meta.get('videoId') or ex.get('platform_id') or ex.get('content_id') or ex.get('id'))
                        if vid:
                            vid_s = str(vid)
                            m = re.search(r'youtube_video_(?P<id>.+)', vid_s)
                            video_id = m.group('id') if m else vid_s
                            if video_id:
                                url = f"https://www.youtube.com/watch?v={video_id}"
                    except Exception:
                        pass

                line = f"- **{title}**\n\n  {trust_assessment}"
                if url:
                    line += f"\n\n  **Link**: {url}"
                else:
                    line += f"\n\n  **Link**: Not available"

                example_lines.append(line)

            examples_md = "\n\n**Examples from this run:**\n" + "\n\n".join(example_lines) + "\n"

        score_based = report_data.get('score_based_ar_pct', 0.0) or 0.0

        # Ensure core classification counts and AR percentages are available
        auth_ratio = report_data.get('authenticity_ratio', {}) or {}
        class_dist = report_data.get('classification_analysis', {}).get('classification_distribution', {}) or {}
        try:
            authentic_items = int(auth_ratio.get('authentic_items', class_dist.get('authentic', 0) or 0))
        except Exception:
            authentic_items = int(class_dist.get('authentic', 0) or 0)
        try:
            suspect_items = int(auth_ratio.get('suspect_items', class_dist.get('suspect', 0) or 0))
        except Exception:
            suspect_items = int(class_dist.get('suspect', 0) or 0)
        try:
            inauthentic_items = int(auth_ratio.get('inauthentic_items', class_dist.get('inauthentic', 0) or 0))
        except Exception:
            inauthentic_items = int(class_dist.get('inauthentic', 0) or 0)

        # Compute core AR% if not provided
        ar_pct = float(auth_ratio.get('authenticity_ratio_pct', 0.0) or 0.0)
        if not ar_pct:
            total_for_ar = float(authentic_items + suspect_items + inauthentic_items) or float(total_items or 0)
            if total_for_ar > 0:
                ar_pct = (float(authentic_items) / total_for_ar) * 100.0
            else:
                ar_pct = 0.0

        extended_ar = float(auth_ratio.get('extended_ar_pct', 0.0) or 0.0)

        # Add a short definitions block to explain Core vs Extended AR
        defs_block = (
            "**Definitions:**\n\n"
            "- Core AR (classification): Percentage of items explicitly classified as 'Authentic' by the classifier (count of Authentic / total * 100).\n"
            "- Score-based AR (mean 5D score): The arithmetic mean of the per-item 5D composite scores (0-100). Useful as a continuous measure.\n"
            "- Extended AR: A blended metric that combines classification counts and score-based signals (rubric-dependent adjustments).\n"
        )

        # Small computation table to show the numbers used to form Core/Extended AR
        computation_table = (
            "\n**Computation (values used):**\n\n"
            f"- Total items = {total_items}\n"
            f"- Authentic = {authentic_items}\n"
            f"- Suspect = {suspect_items}\n"
            f"- Inauthentic = {inauthentic_items}\n"
            f"- Core AR = Authentic / Total * 100 = {ar_pct:.1f}%\n"
            f"- Score-based AR (mean 5D) = {score_based:.1f}%\n"
            f"- Extended AR = rubric-adjusted blend (see AR Analysis section) = {extended_ar:.1f}%\n"
        )

        return f"""## Summary

**Total Content Analyzed:** {total_items:,}

{trust_stack_table}

---

### Key Insights

{interp}

{examples_md}

{visuals_block}

"""

    # ============ AUTHENTICITY RATIO FUNCTIONS (PRESERVED FOR FUTURE USE) ============
    # These functions are preserved but not currently used in Trust Stack reports.
    # See line 289 in _build_markdown_content() to re-enable AR Analysis section.

    def _create_ar_analysis(self, report_data: Dict[str, Any]) -> str:
        """Create Authenticity Ratio analysis section (DISABLED - Trust Stack rebrand)"""
        ar_data = report_data.get('authenticity_ratio', {})
        # Build per-dimension subsections
        dimension_breakdown = report_data.get('dimension_breakdown', {})
        dimension_sections = []
        defs = {
            'provenance': 'How traceable and source-verified the content is.',
            'verification': 'Alignment with verifiable brand or regulatory data.',
            'transparency': 'Clarity of ownership, disclosure, and intent.',
            'coherence': 'Consistency of messaging and tone with known brand assets.',
            'resonance': 'Audience engagement that aligns with brand values.'
        }

        for dim in ['provenance', 'verification', 'transparency', 'coherence', 'resonance']:
            stats = dimension_breakdown.get(dim, {})
            avg = stats.get('average', 0.0)
            lo = stats.get('min', 0.0)
            hi = stats.get('max', 0.0)
            interp = ''
            # Use provided plain-language interpretations from spec where helpful
            if dim == 'provenance':
                interp = 'Moderate provenance indicates some content includes brand-linked metadata or source signals (e.g., official product listings or verified user accounts), but most lacks clear traceability.'
            elif dim == 'verification':
                interp = 'Verification is the weakest pillar. Few posts or listings reference authoritative identifiers (such as verified domains, SSL certificates, or official brand handles).'
            elif dim == 'transparency':
                interp = 'Transparency remains low, likely due to missing disclosure tags or ambiguous authorship.'
            elif dim == 'coherence':
                interp = 'Inconsistent tone or visual style may indicate unofficial reshares or imitations.'
            elif dim == 'resonance':
                interp = 'Resonance is relatively stable but not strongly correlated with authenticity, suggesting popular content may not be brand-originated.'

            section = f"### {dim.title()}\n\n**Definition:** {defs.get(dim)}\n\n**Key Stats:** Average: {avg:.3f} | Range: {lo:.2f}‚Äì{hi:.2f}\n\n**Interpretation:** {interp}\n"
            dimension_sections.append(section)

        dimension_text = '\n'.join(dimension_sections)

        # Summary block
        total_items = ar_data.get('total_items', 0)
        authentic_items = ar_data.get('authentic_items', 0)
        suspect_items = ar_data.get('suspect_items', 0)
        inauthentic_items = ar_data.get('inauthentic_items', 0)

        summary = f"""## Authenticity Ratio Analysis\n\n**Total:** {total_items:,} | **Authentic:** {authentic_items} | **Suspect:** {suspect_items} | **Inauthentic:** {inauthentic_items}\n\n**Core AR:** {ar_data.get('authenticity_ratio_pct', 0.0):.1f}% | **Extended AR:** {ar_data.get('extended_ar_pct', 0.0):.1f}%\n\n{dimension_text}"""

        return summary
    
    def _create_dimension_breakdown(self, report_data: Dict[str, Any]) -> str:
        """Create dimension breakdown section"""
        dimension_data = report_data.get('dimension_breakdown', {})
        
        if not dimension_data:
            return "## 5D Trust Dimensions Analysis\n\n*No dimension data available*"
        
        # Create dimension scores table
        table_rows = ["| Dimension | Average | Min | Max | Std Dev |"]
        table_rows.append("|-----------|---------|-----|-----|---------|")
        
        for dimension, stats in dimension_data.items():
            table_rows.append(f"| {dimension.title()} | {stats.get('average', 0):.3f} | {stats.get('min', 0):.3f} | {stats.get('max', 0):.3f} | {stats.get('std_dev', 0):.3f} |")
        
        dimension_table = "\n".join(table_rows)
        
        # Add dimension descriptions
        descriptions = {
            'provenance': 'Origin clarity, traceability, and metadata completeness',
            'verification': 'Factual accuracy and consistency with trusted sources',
            'transparency': 'Clear disclosures and honest communication',
            'coherence': 'Consistency with brand messaging and professional quality',
            'resonance': 'Cultural fit and authentic engagement patterns'
        }
        
        dimension_details = []
        for dimension, stats in dimension_data.items():
            avg_score = stats.get('average', 0)
            description = descriptions.get(dimension, 'No description available')
            
            # Add performance indicator
            if avg_score >= 0.8:
                indicator = "üü¢ Excellent"
            elif avg_score >= 0.6:
                indicator = "üü° Good"
            elif avg_score >= 0.4:
                indicator = "üü† Moderate"
            else:
                indicator = "üî¥ Poor"
            
            dimension_details.append(f"**{dimension.title()}** ({indicator}): {description}")
        
        return f"""## 5D Trust Dimensions Analysis

### Dimension Scores

{dimension_table}

### Dimension Performance

{chr(10).join(dimension_details)}

### Scoring Methodology

Each dimension is scored on a scale of 0.0 to 1.0:
- **0.8-1.0**: Excellent performance
- **0.6-0.8**: Good performance
- **0.4-0.6**: Moderate performance
- **0.0-0.4**: Poor performance

Each dimension is independently scored and combined to form a comprehensive trust profile, with equal weighting (20% each) across all five dimensions."""
    
    def _create_classification_analysis(self, report_data: Dict[str, Any]) -> str:
        """Create classification analysis section"""
        classification_data = report_data.get('classification_analysis', {})

        if not classification_data:
            return "## Content Trust Classification\n\n*No classification data available*"

        dist = classification_data.get('classification_distribution', {})
        total = sum(dist.values())

        if total == 0:
            return "## Content Trust Classification\n\n*No classification data available*"

        # Create classification summary (using existing keys but new labels)
        high_trust_pct = dist.get('authentic', 0) / total * 100
        moderate_trust_pct = dist.get('suspect', 0) / total * 100
        low_trust_pct = dist.get('inauthentic', 0) / total * 100

        return f"""## Content Trust Classification

### Trust Distribution

| Trust Level | Count | Percentage | Status |
|-------------|-------|------------|---------|
| **High Trust** | {dist.get('authentic', 0):,} | {high_trust_pct:.1f}% | ‚úÖ Strengthens brand |
| **Moderate Trust** | {dist.get('suspect', 0):,} | {moderate_trust_pct:.1f}% | ‚ö†Ô∏è Needs verification |
| **Low Trust** | {dist.get('inauthentic', 0):,} | {low_trust_pct:.1f}% | ‚ùå Requires attention |

### Classification Definitions

- **High Trust**: Content that meets trust standards across all five dimensions with high confidence (score ‚â•0.70)
- **Moderate Trust**: Content that shows mixed trust signals and requires additional verification (score 0.50-0.69)
- **Low Trust**: Content that fails trust criteria or shows weak dimensional scores (score <0.50)

### Action Items by Trust Level

#### üü¢ High Trust Content ({dist.get('authentic', 0):,} items)
- **Action**: Amplify and promote
- **Strategy**: Use as examples of trusted brand engagement
- **Goal**: Increase visibility and reach

#### üü° Moderate Trust Content ({dist.get('suspect', 0):,} items)
- **Action**: Investigate and enhance
- **Strategy**: Apply additional verification and improve weak dimensions
- **Goal**: Elevate to high trust classification or flag for review

#### üî¥ Low Trust Content ({dist.get('inauthentic', 0):,} items)
- **Action**: Review and remediate or remove
- **Strategy**: Address trust deficiencies or report to platform administrators
- **Goal**: Improve trust scores or eliminate from brand ecosystem"""
    
    def _extract_low_trust_items(self, report_data: Dict[str, Any], limit: int = 10) -> List[Dict[str, Any]]:
        """Extract low and moderate trust items with details for LLM analysis"""
        items = report_data.get('items', [])
        appendix = report_data.get('appendix', [])

        # Use appendix if available (more detailed), otherwise use items
        source = appendix if appendix else items

        low_trust = []
        moderate_trust = []

        for item in source:
            # Handle both dict and object types
            if not isinstance(item, dict):
                try:
                    item = {k: getattr(item, k) for k in dir(item) if not k.startswith('_') and not callable(getattr(item, k, None))}
                except Exception:
                    continue

            final_score = float(item.get('final_score', 0.0))
            label = (item.get('label') or '').lower()

            item_data = {
                'url': item.get('url', 'Unknown URL'),
                'title': item.get('title', 'No title'),
                'score': final_score,
                'label': label,
                'source': item.get('source', 'Unknown'),
                'dimension_scores': item.get('dimension_scores', {})
            }

            if label == 'inauthentic' or final_score < 0.50:
                low_trust.append(item_data)
            elif label == 'suspect' or (0.50 <= final_score < 0.70):
                moderate_trust.append(item_data)

        # Sort by score (worst first)
        low_trust.sort(key=lambda x: x['score'])
        moderate_trust.sort(key=lambda x: x['score'])

        return {
            'low_trust': low_trust[:limit],
            'moderate_trust': moderate_trust[:limit]
        }

    def _prepare_recommendation_context(self, report_data: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare comprehensive context for LLM recommendation generation"""
        dimension_breakdown = report_data.get('dimension_breakdown', {})
        classification_data = report_data.get('classification_analysis', {})
        dist = classification_data.get('classification_distribution', {})

        # Calculate dimension scores
        dimension_scores = {}
        weakest_dim = None
        weakest_score = 1.0

        for dim in ['provenance', 'verification', 'transparency', 'coherence', 'resonance']:
            stats = dimension_breakdown.get(dim, {})
            avg = stats.get('average', 0.0)
            dimension_scores[dim] = avg
            if avg < weakest_score:
                weakest_score = avg
                weakest_dim = dim

        avg_all_dims = sum(dimension_scores.values()) / len(dimension_scores) if dimension_scores else 0.0

        # Get problematic items
        problematic_items = self._extract_low_trust_items(report_data)

        return {
            'dimension_scores': dimension_scores,
            'weakest_dimension': weakest_dim,
            'weakest_score': weakest_score,
            'overall_average': avg_all_dims,
            'classification_counts': {
                'high_trust': dist.get('authentic', 0),
                'moderate_trust': dist.get('suspect', 0),
                'low_trust': dist.get('inauthentic', 0),
                'total': sum(dist.values())
            },
            'low_trust_items': problematic_items['low_trust'],
            'moderate_trust_items': problematic_items['moderate_trust'],
            'brand_id': report_data.get('brand_id', 'Unknown Brand')
        }

    def _llm_generate_recommendations(self, context: Dict[str, Any], model: str = 'gpt-4o-mini') -> Optional[str]:
        """Use LLM to generate rich, contextual recommendations

        Args:
            context: Analysis context with dimension scores and problematic items
            model: OpenAI model to use (configurable via --recommendations-model flag)
                   Options: gpt-4o, gpt-4o-mini, gpt-3.5-turbo

        Returns:
            LLM-generated recommendations or None if generation fails
        """
        # Avoid LLM calls during testing
        try:
            import os as _os
            if 'PYTEST_CURRENT_TEST' in _os.environ:
                return None
        except Exception:
            pass

        try:
            from scoring.llm_client import ChatClient
        except Exception:
            return None

        # Build prompt with comprehensive context
        prompt = f"""You are a Trust Stack analyst generating actionable recommendations for brand content authenticity.

BRAND: {context['brand_id']}

TRUST STACK ANALYSIS:
- Overall Trust Average: {context['overall_average']:.3f}
- Weakest Dimension: {context['weakest_dimension']} ({context['weakest_score']:.3f})

DIMENSION SCORES:
"""
        for dim, score in context['dimension_scores'].items():
            status = "‚úÖ" if score >= 0.70 else "‚ö†Ô∏è" if score >= 0.50 else "‚ùå"
            prompt += f"  {status} {dim.title()}: {score:.3f}\n"

        prompt += f"""
CLASSIFICATION COUNTS:
- High Trust: {context['classification_counts']['high_trust']} items
- Moderate Trust: {context['classification_counts']['moderate_trust']} items
- Low Trust: {context['classification_counts']['low_trust']} items
- Total Analyzed: {context['classification_counts']['total']} items

LOW TRUST ITEMS (examples):
"""
        for item in context['low_trust_items'][:5]:
            prompt += f"  - {item['title']} ‚Äî {item['url']}\n"
            prompt += f"    Score: {item['score']:.2f}, Source: {item['source']}\n"

        prompt += """
Generate a comprehensive recommendations report with these EXACT sections:

### Recommended Actions

- Provide 4-6 specific, actionable recommendations based on the data
- Reference actual dimension weaknesses and low-trust items
- Be concrete and measurable (not generic advice)
- Prioritize by impact

### Next Steps (concrete)

**Low Trust items (examples):**
- List 2-3 specific low-trust items with URLs that need immediate attention
- Provide brief context on why they are problematic

- Immediate (1-7 days):
  - List 2-3 concrete actions with specific targets
  - Example: "Review and improve metadata for 5 low-trust Reddit posts"

- Short-term (1-4 weeks):
  - List 2-3 tactical improvements tied to dimension weaknesses
  - Be specific about what to implement

- Medium (4-12 weeks):
  - List 2-3 strategic initiatives for long-term trust improvement
  - Connect to overall trust average goals

### Success Metrics

Provide 4-6 specific, measurable metrics including:
- Concrete targets for dimension improvements (e.g., "Increase transparency from 0.58 to 0.68")
- Classification count goals (e.g., "Reduce low trust items from X to Y")
- Timeline-specific milestones
- Overall trust average improvement target

IMPORTANT: Use the actual data provided. Reference specific numbers, URLs, and dimensions. Be actionable and concrete, not generic."""

        try:
            client = ChatClient()
            response = client.chat(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2000,
                temperature=0.5
            )

            content = response.get('content') or response.get('text')
            if content:
                # Add LLM provenance
                content = f"{content}\n\n*Recommendations generated by {model} based on Trust Stack analysis data*"
            return content

        except Exception as e:
            logger.warning(f"LLM recommendation generation failed: {e}")
            return None

    def _create_recommendations(self, report_data: Dict[str, Any]) -> str:
        """Create recommendations section using LLM for rich, contextual insights"""
        # Prepare context
        context = self._prepare_recommendation_context(report_data)

        # Get model from report_data (set by run_pipeline --recommendations-model flag)
        recommendations_model = report_data.get('recommendations_model', 'gpt-4o-mini')

        # Determine priority level for header
        weakest_score = context['weakest_score']
        if weakest_score >= 0.70:
            priority = "Low"
            focus = "Maintain current trust standards"
        elif weakest_score >= 0.50:
            priority = "Medium"
            focus = f"Strengthen {context['weakest_dimension'].title()} dimension"
        elif weakest_score >= 0.30:
            priority = "High"
            focus = f"Address critical gaps in {context['weakest_dimension'].title()}"
        else:
            priority = "Critical"
            focus = f"Emergency intervention on {context['weakest_dimension'].title()}"

        # Try LLM-enhanced recommendations with specified model
        llm_recommendations = self._llm_generate_recommendations(context, model=recommendations_model)

        if llm_recommendations:
            # LLM succeeded - use rich contextual recommendations
            return f"""## Recommendations

### Priority Level: {priority}
**Focus Area**: {focus}

**Weakest Dimension**: {context['weakest_dimension'].title()} ({weakest_score:.3f})
**Overall Trust Average**: {context['overall_average']:.3f}

{llm_recommendations}"""

        # Fallback to basic recommendations if LLM fails
        dimension_guidance = {
            'provenance': "Improve source attribution, metadata completeness, and traceability",
            'verification': "Strengthen fact-checking, authoritative source validation, and brand alignment",
            'transparency': "Enhance disclosure practices, authorship clarity, and ownership transparency",
            'coherence': "Ensure brand messaging consistency, professional quality, and tone alignment",
            'resonance': "Foster authentic engagement patterns and cultural fit with brand values"
        }

        low_items = context['low_trust_items'][:3]
        low_items_text = "\n".join([f"- {item['title']} ‚Äî {item['url']}" for item in low_items]) if low_items else "- No low trust items found"

        return f"""## Recommendations

### Priority Level: {priority}
**Focus Area**: {focus}

**Weakest Dimension**: {context['weakest_dimension'].title()} ({weakest_score:.3f})
**Overall Trust Average**: {context['overall_average']:.3f}

### Recommended Actions

- Focus on improving {context['weakest_dimension'].title()} scores (currently {weakest_score:.2f})
- {dimension_guidance.get(context['weakest_dimension'], 'Improve trust across all dimensions')}
- Review and remediate {context['classification_counts']['low_trust']} low-trust items
- Implement enhanced verification for {context['classification_counts']['moderate_trust']} moderate-trust items

### Next Steps (concrete)

**Low Trust items (examples):**
{low_items_text}

- Immediate (1-7 days):
  - Review {len(low_items)} low-trust items and develop remediation plan
  - Implement monitoring alerts for {context['weakest_dimension']} dimension

- Short-term (1-4 weeks):
  - Develop {context['weakest_dimension']}-specific content guidelines
  - Train teams on Trust Stack standards

- Medium (4-12 weeks):
  - Establish Trust Stack monitoring dashboards
  - Target overall trust average improvement to {context['overall_average'] + 0.15:.2f}

### Success Metrics

- Improve {context['weakest_dimension'].title()} dimension score from {weakest_score:.2f} to {min(weakest_score + 0.10, 1.0):.2f}
- Reduce low trust items from {context['classification_counts']['low_trust']} to {max(0, context['classification_counts']['low_trust'] - 5)}
- Increase overall Trust average from {context['overall_average']:.2f} to {min(context['overall_average'] + 0.15, 1.0):.2f}
- Achieve minimum 0.60 score across all five dimensions"""
    
    def _create_footer(self, report_data: Dict[str, Any]) -> str:
        """Create report footer"""
        generated_at = report_data.get('generated_at', datetime.now().isoformat())
        brand_id = report_data.get('brand_id', 'Unknown Brand')

        return f"""---

## About This Report

**Trust Stack‚Ñ¢** is a 5-dimensional framework for evaluating brand content authenticity across digital channels.

This report provides actionable insights for brand health and content strategy based on comprehensive trust dimension analysis: **Provenance**, **Verification**, **Transparency**, **Coherence**, and **Resonance**.

### Learn More

- **[Trust Stack Methodology](../docs/AR_METHODOLOGY.md)** - Complete methodology, formulas, and scoring criteria
- **Tool Version**: v1.0
- **Generated**: {generated_at}

---

*This report is confidential and proprietary. For questions or additional analysis, contact the Trust Stack team.*"""


    def _create_appendix(self, report_data: Dict[str, Any]) -> str:
        """Render an appendix with per-item diagnostics if available"""
        appendix = report_data.get('appendix', []) or []

        # If pipeline didn't attach a rich appendix, fall back to the items list
        # which contains the minimal per-item summaries (meta, final_score, label).
        items_fallback = report_data.get('items', []) or []
        if not appendix and not items_fallback:
            return "## Appendix: Per-item Diagnostics\n\n*No per-item diagnostics available for this run.*"

    # Support a mode where only the most egregious examples are shown.
        egregious_only = bool(report_data.get('appendix_egregious_only') or report_data.get('appendix_mode') == 'egregious')
        # Default to showing ALL items (None = no limit), but allow override
        limit = report_data.get('appendix_limit', None)
        if limit is not None:
            limit = int(limit)

        if egregious_only:
            # Define egregious as items labeled 'inauthentic' or with very low final_score
            def is_egregious(it: Dict[str, Any]) -> bool:
                lbl = (it.get('label') or '').lower()
                final = float(it.get('final_score') or 0.0)
                return lbl == 'inauthentic' or final <= 40.0

            filtered = [it for it in appendix if is_egregious(it)]
            # Sort by ascending final score (worst first)
            filtered.sort(key=lambda x: float(x.get('final_score') or 0.0))
            appendix = filtered[:limit] if limit else filtered
        elif limit:
            # Apply limit if specified
            appendix = appendix[:limit] if appendix else items_fallback[:limit]

        # Choose which source to iterate: prefer full appendix entries, otherwise fall back to items.
        render_source = appendix if appendix else items_fallback

        total_count = len(render_source)
        lines = [f"## Appendix: Per-item Diagnostics\n\nThis appendix lists all {total_count} analyzed items with detailed diagnostics including source, title, description, visited URL, final score, and rationale.\n"]

        for item in render_source:
            # Normalize access to fields whether item came from appendix or items list
            # Some pipeline paths pass objects (not dicts). Coerce to a dict-like mapping
            if not isinstance(item, dict):
                try:
                    class _AttrDict(dict):
                        def __init__(self, obj):
                            super().__init__()
                            for k in dir(obj):
                                if k.startswith('_'):
                                    continue
                                try:
                                    v = getattr(obj, k)
                                except Exception:
                                    continue
                                if callable(v):
                                    continue
                                try:
                                    self[k] = v
                                except Exception:
                                    continue
                    item = _AttrDict(item)
                except Exception:
                    # fallback: wrap minimal representation
                    try:
                        item = dict(item)
                    except Exception:
                        item = {}

            cid = item.get('content_id') or item.get('id') or 'unknown'
            meta = item.get('meta') or {}
            # meta might be a JSON string in some cases
            if isinstance(meta, str):
                try:
                    import json as _json
                    meta = _json.loads(meta) if meta else {}
                except Exception:
                    meta = {}

            # If scorer attached an 'orig_meta' wrapper (preserved original fetch meta),
            # prefer values from there when top-level meta fields are missing.
            try:
                orig_meta = meta.get('orig_meta') if isinstance(meta, dict) else None
                if isinstance(orig_meta, str):
                    try:
                        import json as _json
                        orig_meta = _json.loads(orig_meta)
                    except Exception:
                        orig_meta = None
            except Exception:
                orig_meta = None

            if isinstance(orig_meta, dict):
                # backfill common fields from orig_meta if missing
                for k in ('title', 'name', 'description', 'snippet', 'body', 'source_url', 'url', 'terms', 'privacy'):
                    if k not in meta or not meta.get(k):
                        try:
                            if orig_meta.get(k):
                                meta[k] = orig_meta.get(k)
                        except Exception:
                            continue

            # Extract title: check item first, then meta fields
            title = (
                item.get('title') or  # Check top-level title field first
                meta.get('title') or
                meta.get('name') or
                meta.get('headline') or
                meta.get('og:title') or
                (item.get('source') or '') + ' - ' + (cid or '')  # Fallback to source-id
            )

            # YouTube-specific title fallback for appendix entries
            if not title or title == ((item.get('source') or '') + ' - ' + (cid or '')):
                try:
                    visited = meta.get('source_url') or meta.get('url') or ''
                    if visited and ('youtube.com' in visited or 'youtu.be' in visited):
                        ch = meta.get('channel_title') or meta.get('publisher') or meta.get('author')
                        vid = None
                        m = re.search(r'(?:v=|youtu\.be/)([A-Za-z0-9_\-]{6,})', visited)
                        if m:
                            vid = m.group(1)
                        else:
                            mm = re.search(r'youtube_video_(?P<id>.+)', str(cid))
                            if mm:
                                vid = mm.group('id')
                        if ch:
                            title = f"YouTube: {ch}"
                        elif vid:
                            title = f"YouTube video {vid}"
                except Exception:
                    pass

                # If still no title (or still the generic source-id fallback) but content id encodes a youtube id, use that
                if not title or title == ((item.get('source') or '') + ' - ' + (cid or '')):
                    try:
                        mm2 = re.search(r'youtube_video_(?P<id>.+)', str(cid))
                        if mm2:
                            vid2 = mm2.group('id')
                            if vid2:
                                title = f"YouTube video {vid2}"
                    except Exception:
                        pass

            # Get visited URL - only show real URLs, not platform IDs
            visited_url = meta.get('source_url') or meta.get('url') or meta.get('source_link') or ''

            # Validate URL - must be a proper URL
            if visited_url and not (visited_url.startswith('http://') or visited_url.startswith('https://')):
                # Check if it's a platform ID (no dots, no slashes) - if so, clear it
                if '.' not in visited_url and '/' not in visited_url:
                    visited_url = ''  # Clear platform IDs like "z1fgz5db-I4" or "1oiqgdr"

            # If no valid URL, try to extract from meta/body text
            if not visited_url:
                try:
                    import json as _json
                    body_candidate = meta.get('body') or item.get('body') or ''
                    combined = ''
                    try:
                        combined = _json.dumps(meta) if meta else ''
                    except Exception:
                        combined = str(meta)
                    if not combined and body_candidate:
                        combined = body_candidate
                    # Look for http(s) links
                    m = re.search(r"https?://[^\s\)\]\'>]+", combined or '')
                    if m:
                        visited_url = m.group(0)
                    else:
                        # Fallback: construct YouTube URL from content id or platform id if present
                        try:
                            vid = item.get('platform_id') or item.get('content_id') or meta.get('video_id') or meta.get('videoId')
                            if vid:
                                vs = str(vid)
                                mm = re.search(r'youtube_video_(?P<id>.+)', vs)
                                vid_id = mm.group('id') if mm else vs
                                if vid_id and re.match(r'^[A-Za-z0-9_\-]{6,}$', vid_id):
                                    visited_url = f"https://www.youtube.com/watch?v={vid_id}"
                        except Exception:
                            pass
                except Exception:
                    pass

            score = item.get('final_score') if item.get('final_score') is not None else item.get('final', None) or 0.0

            # Convert label to trust-based terminology
            raw_label = (item.get('label') or item.get('class_label') or '').lower()
            if raw_label == 'authentic':
                label = 'High Trust'
            elif raw_label == 'suspect':
                label = 'Moderate Trust'
            elif raw_label == 'inauthentic':
                label = 'Low Trust'
            else:
                label = (item.get('label') or item.get('class_label') or '').title()

            # Build a natural-language rationale using dimension signals, applied rules, and metadata cues
            rationale_sentences = []
            dims = item.get('dimension_scores') or {}
            if isinstance(dims, dict) and dims:
                # Identify weakest dimensions
                try:
                    dim_items = [(k, float(v)) for k, v in dims.items() if v is not None]
                    dim_items.sort(key=lambda x: x[1])
                    weakest = dim_items[:2]
                    best = dim_items[-1] if dim_items else None
                    if weakest:
                        w_names = ', '.join([f"{n.title()} ({v:.2f})" for n, v in weakest])
                        rationale_sentences.append(f"Low signals in {w_names} contributed to the lower score.")
                    if best and best[1] >= 0.8:
                        rationale_sentences.append(f"Strong {best[0].title()} signal ({best[1]:.2f}) partially offset weaknesses.")
                except Exception:
                    pass

            # Meta-based cues (missing Terms/Privacy or missing meta tags)
            try:
                if isinstance(meta, dict):
                    # Check for presence of common site metadata
                    has_terms = bool(meta.get('terms') or meta.get('terms_url'))
                    has_privacy = bool(meta.get('privacy') or meta.get('privacy_url'))
                    has_og = bool(meta.get('og:title') or meta.get('og:description') or meta.get('og'))
                    # If explicit meta flags are missing, scan the body text for common footer links/phrases
                    if not has_terms or not has_privacy:
                        body_text = ''
                        try:
                            body_text = (meta.get('body') or item.get('body') or '')
                        except Exception:
                            body_text = ''
                        if body_text:
                            lowered = body_text.lower()
                            # Common English phrases
                            if not has_terms and ('terms of service' in lowered or 'terms & conditions' in lowered or 'terms and conditions' in lowered or '/terms' in lowered or 'terms' in lowered):
                                has_terms = True
                            if not has_privacy and ('privacy policy' in lowered or 'privacy' in lowered or '/privacy' in lowered or 'politique de confidentialit√©' in lowered):
                                has_privacy = True
                    if not has_terms and not has_privacy:
                        rationale_sentences.append('The site lacks visible Terms/Privacy links which reduces trust signals.')
                    if not has_og:
                        rationale_sentences.append('Missing open-graph metadata reduced the detectable content richness.')
            except Exception:
                pass

            # Applied rules and reasons
            rules = item.get('applied_rules') or []
            if rules:
                try:
                    rule_msgs = []
                    for r in rules:
                        rid = r.get('id') or r.get('rule') or 'rule'
                        eff = r.get('effect', '')
                        reason = r.get('reason') or ''
                        rule_msgs.append(f"{rid} {eff}{(': ' + reason) if reason else ''}")
                    if rule_msgs:
                        rationale_sentences.append('Applied rules: ' + '; '.join(rule_msgs))
                except Exception:
                    pass

            # LLM annotations if present
            try:
                if isinstance(meta, dict) and meta.get('_llm_classification'):
                    llm = meta.get('_llm_classification')
                    lab = llm.get('label') or ''
                    conf = llm.get('confidence')
                    rationale_sentences.append(f"LLM classified this item as {lab} (conf={conf}).")
            except Exception:
                pass

            rationale = ' '.join(rationale_sentences) if rationale_sentences else ''

            # Build trust assessment explanation instead of content description
            trust_assessment = ""
            if dims:
                try:
                    # Parse dimension scores
                    dims_parsed = {k: float(v) for k, v in dims.items() if v is not None}
                    if dims_parsed:
                        sorted_dims = sorted(dims_parsed.items(), key=lambda x: x[1])
                        weakest_dims = sorted_dims[:2] if len(sorted_dims) >= 2 else sorted_dims
                        strongest_dims = sorted(dims_parsed.items(), key=lambda x: x[1], reverse=True)[:1]

                        # Determine trust level
                        if float(score) >= 0.70:
                            trust_level = "High Trust"
                        elif float(score) >= 0.50:
                            trust_level = "Moderate Trust"
                        else:
                            trust_level = "Low Trust"

                        trust_assessment = f"{trust_level} rating based on score of {float(score):.2f}. "

                        if weakest_dims:
                            weak_str = ', '.join([f"{k.title()} ({v:.2f})" for k, v in weakest_dims])
                            trust_assessment += f"Weaker dimensions: {weak_str}. "

                        if strongest_dims:
                            strong_str = ', '.join([f"{k.title()} ({v:.2f})" for k, v in strongest_dims])
                            trust_assessment += f"Strongest: {strong_str}."
                except Exception:
                    trust_assessment = f"Score: {float(score):.2f}"
            else:
                trust_assessment = f"Score: {float(score):.2f}"

            # Format visited URL
            visited_display = visited_url if visited_url else 'Not available'

            # Determine source string for display (fall back to common fields)
            source = item.get('source') or meta.get('source') or item.get('src') or item.get('platform') or 'unknown'
            try:
                source = str(source)
            except Exception:
                source = 'unknown'

            lines.append(
                f"### {title}\n\n"
                f"- **Source**: {source.title()}\n"
                f"- **Title**: {title}\n"
                f"- **Trust Assessment**: {trust_assessment}\n"
                f"- **Visited URL**: {visited_display}\n"
                f"- **Score**: {float(score):.2f} | **Trust Level**: {label}\n"
                f"- **Rationale**: {rationale if rationale else 'No detailed rationale available.'}\n\n---\n"
            )

        return "\n".join(lines)

    # --- Visual helpers -------------------------------------------------
    def _ensure_output_dir(self, report_data: Dict[str, Any]) -> str:
        out_dir = report_data.get('output_dir', './output')
        os.makedirs(out_dir, exist_ok=True)
        return out_dir

    def _create_dimension_heatmap(self, report_data: Dict[str, Any]) -> str:
        """Create a simple heatmap of the five-dimension averages. Returns image path or empty string."""
        dims = report_data.get('dimension_breakdown', {})
        if not dims:
            return ""

        labels = ['Provenance', 'Verification', 'Transparency', 'Coherence', 'Resonance']
        values = [dims.get(k.lower(), {}).get('average', 0.0) for k in labels]
        if sum(values) == 0:
            return ""

        out_dir = self._ensure_output_dir(report_data)
        img_path = os.path.join(out_dir, f"heatmap_{report_data.get('run_id','run')}.png")

        # heatmap as a 1x5 colored bar
        fig, ax = plt.subplots(figsize=(6, 1.5))
        cmap = plt.get_cmap('RdYlGn')
        norm = plt.Normalize(0, 1)
        ax.imshow([values], aspect='auto', cmap=cmap, norm=norm)
        ax.set_yticks([])
        ax.set_xticks(range(len(labels)))
        ax.set_xticklabels(labels, rotation=45, ha='right')
        ax.set_title('5D Trust Dimensions (average scores)')
        plt.tight_layout()
        fig.savefig(img_path, dpi=150)
        plt.close(fig)
        return img_path

    def _create_trendline(self, report_data: Dict[str, Any]) -> str:
        """Attempt to build a simple AR trendline by scanning previous markdown reports in output dir."""
        out_dir = self._ensure_output_dir(report_data)
        # Scan output dir for existing markdown reports with run IDs and AR lines
        pattern = re.compile(r"Core Authenticity Ratio:\*\* (\d+\.?\d*)%")
        runs = []  # list of (timestamp, ar_pct)
        for fname in os.listdir(out_dir):
            if not fname.endswith('.md'):
                continue
            path = os.path.join(out_dir, fname)
            try:
                with open(path, 'r', encoding='utf-8') as fh:
                    content = fh.read()
                m = pattern.search(content)
                if m:
                    ar = float(m.group(1))
                    # try to extract timestamp from filename if included
                    ts_match = re.search(r'run_(\d{8}_\d{6}_[0-9a-f]+)', fname)
                    ts = ts_match.group(1) if ts_match else fname
                    runs.append((ts, ar))
            except Exception:
                continue

        # include current run at the end
        current_ar = report_data.get('authenticity_ratio', {}).get('authenticity_ratio_pct', None)
        if current_ar is None:
            return ""

        # sort by filename (best-effort chronological ordering)
        if runs:
            runs_sorted = sorted(runs, key=lambda x: x[0])
            xs = list(range(len(runs_sorted)))
            ys = [r[1] for r in runs_sorted]
            xs.append(len(xs))
            ys.append(current_ar)
        else:
            # no historical runs
            return ""

        img_path = os.path.join(out_dir, f"ar_trend_{report_data.get('run_id','run')}.png")
        fig, ax = plt.subplots(figsize=(6, 2.5))
        ax.plot(xs, ys, marker='o')
        ax.set_xlabel('Run (historic -> latest)')
        ax.set_ylabel('Core AR (%)')
        ax.set_title('Authenticity Ratio Trend')
        ax.grid(True, linestyle='--', alpha=0.4)
        plt.tight_layout()
        fig.savefig(img_path, dpi=150)
        plt.close(fig)
        return img_path

    def _create_channel_breakdown(self, report_data: Dict[str, Any]) -> str:
        """Create channel breakdown bar chart if classification includes source channel info."""
        class_analysis = report_data.get('classification_analysis', {})
        if not class_analysis:
            return ""
        # look for per-channel distribution
        per_channel = class_analysis.get('by_channel', {})
        if not per_channel:
            return ""

        labels = list(per_channel.keys())
        counts = [per_channel[k] for k in labels]
        if sum(counts) == 0:
            return ""

        out_dir = self._ensure_output_dir(report_data)
        img_path = os.path.join(out_dir, f"channel_breakdown_{report_data.get('run_id','run')}.png")

        fig, ax = plt.subplots(figsize=(6, 3))
        ax.bar(labels, counts, color='tab:blue')
        ax.set_ylabel('Count')
        ax.set_title('Per-Channel Classification Counts')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        fig.savefig(img_path, dpi=150)
        plt.close(fig)
        return img_path

    def _create_content_type_breakdown(self, report_data: Dict[str, Any]) -> str:
        """Create pie and bar charts for content-type percentage breakdown."""
        ctype = report_data.get('content_type_breakdown_pct', {})
        if not ctype:
            return ""

        out_dir = self._ensure_output_dir(report_data)
        pie_path = os.path.join(out_dir, f"content_type_pie_{report_data.get('run_id','run')}.png")
        bar_path = os.path.join(out_dir, f"content_type_bar_{report_data.get('run_id','run')}.png")

        labels = list(ctype.keys())
        values = [ctype[k] for k in labels]
        if sum(values) == 0:
            return ""

        # Pie chart
        fig1, ax1 = plt.subplots(figsize=(5, 3))
        ax1.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)
        ax1.axis('equal')
        ax1.set_title('Content Type Distribution')
        plt.tight_layout()
        fig1.savefig(pie_path, dpi=150)
        plt.close(fig1)

        # Bar chart
        fig2, ax2 = plt.subplots(figsize=(6, 3))
        ax2.bar(labels, values, color='tab:green')
        ax2.set_ylabel('Percentage')
        ax2.set_title('Content Type Percentage')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        fig2.savefig(bar_path, dpi=150)
        plt.close(fig2)

        # Return the pie path (used in the executive visuals); the bar is saved alongside
        return pie_path
