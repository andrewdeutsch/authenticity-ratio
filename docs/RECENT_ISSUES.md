# Recent Issues (captured 2025-10-27)

This document summarizes the recent technical issues we have been contending with, their current status, and recommended next steps.

---

1) Brave Search API — 422 VALIDATION for large count
- Symptom: Brave API returns HTTP 422 when `count` is too large (e.g., 30), causing ingestion to fail or return 0 items.
- Files involved: `ingestion/brave_search.py`
- Status: Resolved / mitigated
- Fix/Workaround: Added `BRAVE_API_MAX_COUNT` env var and cap `params['count']` to `min(requested, max_api_count)`. Verified by re-running the pipeline.
- Next steps: Consider documenting per-source limits or emitting a runtime warning if requested `count` > cap.


2) Appendix items missing Title / Description / Visited URL
- Symptom: Appendix entries rendered as "*No description available*" or Visited URL "*N/A*".
- Files involved: `reporting/markdown_generator.py`, `reporting/pdf_generator.py`, `scoring/pipeline.py`
- Status: Mostly mitigated (improvements deployed)
- Fixes implemented:
  - Backfill `meta` from `orig_meta` when available.
  - Title fallback chain: meta.title → og:title/headline → body snippet → content_id.
  - Description fallback chain: LLM summarize (if enabled) → extractive summarizer → synthesized short text (domain/terms/privacy/content_type).
  - Visited URL fallback chain: source_url/url → platform_id → terms/privacy links → regex extraction from meta/body.
- Remaining work: Add unit tests to ensure consistent fallbacks; optionally move URL extraction earlier (normalization) to keep reporting simpler.


3) Inconsistent or missing LLM provenance labeling
- Symptom: LLM-generated descriptions sometimes include provenance "(Generated by <model>)", but not guaranteed across all report code paths.
- Files involved: `reporting/markdown_generator.py`, `reporting/pdf_generator.py`
- Status: In-progress (partially applied)
- Fix/Workaround: Some code paths append provenance already. We should centralize provenance appending in a helper and use it wherever LLM-generated text is created.
- Next steps: Add a helper to canonicalize provenance formatting and add a unit test.


4) Undefined variable bug in scoring enrichment (`weights`)
- Symptom: `scoring/pipeline.py` attempted to compute `final_score` using a `weights` variable not in scope, causing lint/compile issues during edits.
- Files involved: `scoring/pipeline.py`
- Status: Fixed
- Fix: Switched final_score computation to the mean of the five dimension scores in the enrichment step to avoid undefined references.
- Next steps: If weighted final scores are desired, introduce a config-driven `weights` constant and validate scope.


5) Regex scoping / NameError during last-resort URL extraction edits
- Symptom: Early patches introduced import/scoping errors (e.g., `re` used inside nested scopes without module-level import or misquoted regex patterns).
- Files involved: `reporting/markdown_generator.py`
- Status: Fixed
- Fix: Moved `re` import to module level and corrected regex quoting; validated with a pipeline run.


6) Placeholder / missing production LLM client
- Symptom: `scoring/llm_client.ChatClient` was a mock or placeholder raising ImportError, so LLM summarization falls back to extractive methods unless a real client is provided.
- Files involved: `scoring/llm_client.py`, `scoring/scorer.py`, `reporting/markdown_generator.py`
- Status: Outstanding
- Next steps: Implement an OpenAI-backed `ChatClient` (configurable model, default `gpt-3.5-turbo`). Document expected env vars (e.g., `OPENAI_API_KEY`) and add a small smoke test using mocks or a real key.


7) Missing unit tests for summarizer fallbacks & appendix behavior
- Symptom: New fallback behaviors are not covered by tests.
- Files involved: `tests/` (new test stubs exist but need implementation)
- Status: Partially started (test stubs were added)
- Next steps: Implement pytest tests for:
  - `_llm_summarize` fallback behavior (mock ChatClient)
  - Extractive summarizer outcomes for short/empty content
  - Appendix enrichment using `ContentScores.meta` / `orig_meta` backfill
  - Provenance label presence when LLM is enabled


8) Robots.txt skipping some social platform pages during Brave ingestion
- Symptom: Brave search includes Instagram, Facebook, X results which are skipped due to robots.txt; reduces fetched items.
- Files involved: `ingestion/brave_search.py`, `ingestion/normalizer.py`
- Status: Observed/expected behavior (not a bug)
- Next steps: Document expected skip behavior and consider reporting counts of skipped vs. fetched items.


9) AR calculation / triage tuning
- Symptom: Triage demotes items before scoring which can affect recall; `max_llm_items` and thresholds may need tuning for large runs.
- Files involved: `scoring/triage.py`, `scoring/pipeline.py`
- Status: Operational; requires periodic tuning
- Next steps: Add a small benchmark job or script to evaluate triage thresholds on representative datasets and surface precision/recall tradeoffs.


---

If you want, I can: 
- (A) add the provenance helper and update both report generators (quick),
- (B) implement a production OpenAI `ChatClient` and run a smoke test (requires API key in env), or
- (C) implement the pytest tests mentioned above (mocking LLMs to keep tests fast).

File location: `docs/RECENT_ISSUES.md`

