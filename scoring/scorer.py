"""
5D Trust Dimensions scorer for Trust Stack Rating tool
Scores content on Provenance, Verification, Transparency, Coherence, Resonance
Integrates with TrustStackAttributeDetector for comprehensive ratings
"""

from typing import Dict, Any, List, Optional
import logging
import json
from dataclasses import dataclass

from config.settings import SETTINGS
from data.models import NormalizedContent, ContentScores, DetectedAttribute
from scoring.attribute_detector import TrustStackAttributeDetector
from scoring.scoring_llm_client import LLMScoringClient

logger = logging.getLogger(__name__)

@dataclass
class DimensionScores:
    """Individual dimension scores"""
    provenance: float
    verification: float
    transparency: float
    coherence: float
    resonance: float

class ContentScorer:
    """
    Scores content on 5D Trust Dimensions
    Combines LLM-based scoring with Trust Stack attribute detection
    """

    def __init__(self, use_attribute_detection: bool = True):
        """
        Initialize scorer

        Args:
            use_attribute_detection: If True, combine LLM scores with attribute detection
        """
        # Initialize LLM scoring client
        self.llm_client = LLMScoringClient()
        self.rubric_version = SETTINGS['rubric_version']
        self.use_attribute_detection = use_attribute_detection

        # Initialize attribute detector if enabled
        if self.use_attribute_detection:
            try:
                self.attribute_detector = TrustStackAttributeDetector()
                logger.info(f"Attribute detector initialized with {len(self.attribute_detector.attributes)} attributes")
            except Exception as e:
                logger.warning(f"Could not initialize attribute detector: {e}. Falling back to LLM-only scoring.")
                self.use_attribute_detection = False
                self.attribute_detector = None
        else:
            self.attribute_detector = None
    
    def score_content(self, content: NormalizedContent, brand_context: Dict[str, Any]) -> DimensionScores:
        """
        Score content on all 5 dimensions

        Args:
            content: Content to score
            brand_context: Brand-specific context and keywords
        """
        logger.debug(f"Scoring content {content.content_id}")

        try:
            # Get LLM scores for each dimension
            provenance_score = self._score_provenance(content, brand_context)
            verification_score = self._score_verification(content, brand_context)
            transparency_score = self._score_transparency(content, brand_context)
            coherence_score = self._score_coherence(content, brand_context)
            resonance_score = self._score_resonance(content, brand_context)

            return DimensionScores(
                provenance=provenance_score,
                verification=verification_score,
                transparency=transparency_score,
                coherence=coherence_score,
                resonance=resonance_score
            )

        except Exception as e:
            logger.error(f"Error scoring content {content.content_id}: {e}")
            # Return neutral scores on error
            return DimensionScores(0.5, 0.5, 0.5, 0.5, 0.5)
    
    def _score_provenance(self, content: NormalizedContent, brand_context: Dict[str, Any]) -> float:
        """Score Provenance dimension: origin, traceability, metadata"""
        
        prompt = f"""
        Score the PROVENANCE of this content on a scale of 0.0 to 1.0.
        
        Provenance evaluates: origin clarity, traceability, metadata completeness
        
        Content:
        Title: {content.title}
        Body: {content.body}
        Author: {content.author}
        Source: {content.src}
        Platform ID: {content.platform_id}
        
        Brand Context: {brand_context.get('keywords', [])}
        
        Scoring criteria:
        - 0.8-1.0: Clear origin, verifiable author, complete metadata
        - 0.6-0.8: Good origin info, some metadata present
        - 0.4-0.6: Basic origin info, limited metadata
        - 0.2-0.4: Unclear origin, minimal metadata
        - 0.0-0.2: No clear origin, no verifiable metadata
        
        Return only a number between 0.0 and 1.0:
        """
        
        return self._get_llm_score(prompt)
    
    def _score_verification(self, content: NormalizedContent, brand_context: Dict[str, Any]) -> float:
        """Score Verification dimension: factual accuracy vs trusted DBs"""
        
        # Detect if content is from brand's own domain
        content_url = getattr(content, 'url', '').lower()
        brand_keywords = [kw.lower() for kw in brand_context.get('keywords', [])]
        is_brand_owned = any(keyword in content_url for keyword in brand_keywords if keyword)
        
        # Adjust verification criteria based on content ownership
        if is_brand_owned:
            ownership_guidance = """
            CONTENT OWNERSHIP: Brand-Owned Content (Official Brand Domain)
            
            ADJUSTED VERIFICATION CRITERIA:
            - Brand statements about their OWN products/services are AUTHORITATIVE (not unverified)
            - DO NOT flag self-descriptive claims like "Our platform offers X feature"
            - ONLY flag these types of unverified claims:
              1. Statistical claims without sources ("99% customer satisfaction", "10x faster")
              2. Comparative claims without proof ("#1 in industry", "better than competitors")
              3. External facts needing citations ("According to studies...", "Research shows...")
              4. Extraordinary claims that seem suspicious
            
            EXAMPLES FOR BRAND-OWNED CONTENT:
            - NOT unverified: "Mastercard Engage offers a directory of approved specialists"
            - NOT unverified: "Our API enables secure payment processing"
            - UNVERIFIED: "We have 99% customer satisfaction" (needs source)
            - UNVERIFIED: "We're the #1 payment company globally" (needs proof)
            """
        else:
            ownership_guidance = """
            CONTENT OWNERSHIP: Third-Party Content
            
            STANDARD VERIFICATION CRITERIA:
            - Apply strict verification standards
            - Flag all factual claims about the brand without sources
            - Flag statistics, rankings, and comparative claims
            - Require citations for all substantive claims
            """
        
        prompt = f"""
        Score the VERIFICATION of this content and identify specific issues.
        
        {ownership_guidance}
        
        Verification evaluates: factual accuracy, consistency with known facts
        
        Content:
        Title: {content.title}
        Body: {content.body[:2000]}
        URL: {content_url}
        
        Brand Context: {brand_context.get('keywords', [])}
        
        CRITICAL REQUIREMENTS:
        1. For EACH issue, provide an EXACT QUOTE from the content as evidence
        2. Do NOT report issues you cannot support with specific text from the content
        3. Include a confidence score (0.0-1.0) for each issue
        4. Only report issues with confidence >= 0.7
        
        EXAMPLES:
        
        Example 1 - High Verification (0.9):
        Content: "According to the Federal Reserve's 2023 report, inflation decreased to 3.2%."
        Response: {{"score": 0.9, "issues": []}}
        
        Example 2 - Low Verification (0.3):
        Content: "We are the world's #1 payment company with 99% customer satisfaction."
        Response: {{
            "score": 0.3,
            "issues": [
                {{
                    "type": "unverified_claims",
                    "confidence": 0.9,
                    "severity": "high",
                    "evidence": "EXACT QUOTE: 'world's #1 payment company with 99% customer satisfaction'",
                    "suggestion": "Add citations for ranking and satisfaction claims"
                }}
            ]
        }}
        
        Respond with JSON in this exact format:
        {{
            "score": 0.5,
            "issues": [
                {{
                    "type": "unverified_claims",
                    "confidence": 0.85,
                    "severity": "high",
                    "evidence": "EXACT QUOTE: 'specific text from content'",
                    "suggestion": "Add citations to authoritative sources"
                }}
            ]
        }}
        
        Scoring criteria:
        - 0.8-1.0: Highly verifiable facts, consistent with known information
        - 0.6-0.8: Mostly accurate, minor inconsistencies
        - 0.4-0.6: Some accurate info, some questionable claims
        - 0.2-0.4: Several inaccuracies or unverifiable claims
        - 0.0-0.2: Major inaccuracies or completely unverifiable
        
        Common verification issues (only report if you can quote specific text):
        - "unverified_claims": Claims without sources (quote the claim)
        - "fake_engagement": Suspicious engagement patterns (quote suspicious metrics)
        - "unlabeled_ads": Sponsored content without disclosure (quote promotional language)
        
        Return valid JSON with score (0.0-1.0) and issues array. ONLY include issues with exact quotes and confidence >= 0.7.
        """
        
        result = self._get_llm_score_with_reasoning(prompt)
        
        # Filter out low-confidence issues
        issues = result.get('issues', [])
        filtered_issues = []
        
        # DIAGNOSTIC: Log all verification issues before filtering
        if issues:
            logger.info(f"[VERIFICATION DIAGNOSTIC] LLM detected {len(issues)} verification issues for {content.content_id[:30]}...")
            for issue in issues:
                logger.info(f"  - Type: {issue.get('type')}, Confidence: {issue.get('confidence', 0.0):.2f}, Severity: {issue.get('severity')}")
        
        for issue in issues:
            confidence = issue.get('confidence', 0.0)
            if confidence >= 0.7:
                filtered_issues.append(issue)
            else:
                logger.info(f"[VERIFICATION DIAGNOSTIC] Filtered low-confidence issue: {issue.get('type')} (confidence={confidence:.2f})")
        
        # Store LLM-identified issues in content metadata for later merging
        if not hasattr(content, '_llm_issues'):
            content._llm_issues = {}
        content._llm_issues['verification'] = filtered_issues
        
        # DIAGNOSTIC: Log final count after filtering
        logger.info(f"[VERIFICATION DIAGNOSTIC] After filtering: {len(filtered_issues)} verification issues stored for merging")
        
        base_score = result.get('score', 0.5)
        
        # Apply content-type multiplier from rubric configuration
        content_type = self._determine_content_type(content)
        multiplier = self._get_score_multiplier('verification', content_type)
        
        if multiplier != 1.0:
            adjusted_score = min(1.0, base_score * multiplier)
            logger.info(f"Verification scoring for {content.content_id[:20]}...")
            logger.info(f"  Content type: {content_type}")
            logger.info(f"  Base LLM score: {base_score:.3f} ({base_score*100:.1f}%)")
            logger.info(f"  Multiplier applied: {multiplier:.2f}x")
            logger.info(f"  Adjusted score: {adjusted_score:.3f} ({adjusted_score*100:.1f}%)")
            return adjusted_score
        
        logger.debug(f"Verification score for {content_type}: {base_score:.3f} (no multiplier)")
        return base_score
    
    def _score_transparency(self, content: NormalizedContent, brand_context: Dict[str, Any]) -> float:
        """Score Transparency dimension: disclosures, clarity"""
        
        prompt = f"""
        Score the TRANSPARENCY of this content and identify specific issues.
        
        Transparency evaluates: clear disclosures, honest communication, no hidden agendas
        
        Content:
        Title: {content.title}
        Body: {content.body}
        Author: {content.author}
        
        Respond with JSON in this exact format:
        {{
            "score": 0.6,
            "issues": [
                {{
                    "type": "missing_privacy_policy",
                    "severity": "medium",
                    "evidence": "No privacy policy link found",
                    "suggestion": "Add privacy policy link to footer"
                }}
            ]
        }}
        
        Scoring criteria:
        - 0.8-1.0: Clear disclosures, honest communication, transparent intent
        - 0.6-0.8: Mostly transparent, minor omissions
        - 0.4-0.6: Some transparency, some unclear aspects
        - 0.2-0.4: Limited transparency, hidden elements
        - 0.0-0.2: No transparency, deceptive or manipulative
        
        Common transparency issues to check for:
        - Missing privacy policy links
        - Unclear AI-generated content disclosure
        - Missing data source citations
        - Hidden sponsored content
        
        Return valid JSON with score (0.0-1.0) and issues array.
        """
        
        result = self._get_llm_score_with_reasoning(prompt)
        
        # Store LLM-identified issues in content metadata for later merging
        if not hasattr(content, '_llm_issues'):
            content._llm_issues = {}
        content._llm_issues['transparency'] = result.get('issues', [])
        
        return result.get('score', 0.5)
    
    def _score_coherence(self, content: NormalizedContent, brand_context: Dict[str, Any]) -> float:
        """Score Coherence dimension: consistency across channels with brand guidelines"""
        
        # Check if user wants to use guidelines (from session state/brand context)
        use_guidelines = brand_context.get('use_guidelines', True)  # Default True for backward compatibility
        
        brand_guidelines = None
        if use_guidelines:
            # Load brand guidelines if available
            brand_id = brand_context.get('brand_name', '').lower().strip().replace(' ', '_')
            brand_guidelines = self._load_brand_guidelines(brand_id)
            
            if brand_guidelines:
                logger.info(f"Using brand guidelines for {brand_id} in coherence scoring ({len(brand_guidelines)} chars)")
            else:
                logger.info(f"No guidelines found for {brand_id}, using generic coherence standards")
        else:
            logger.info("Brand guidelines disabled by user preference")
        
        # Detect content type to adjust scoring criteria
        content_type = self._determine_content_type(content)
        
        # Build context guidance for the feedback step
        if brand_guidelines:
            # Use brand-specific guidelines
            guidelines_preview = brand_guidelines[:1500]  # First 1500 chars
            context_guidance = f"""
            BRAND GUIDELINES FOR {brand_id.upper()}:
            
            {guidelines_preview}
            
            {'... [guidelines truncated]' if len(brand_guidelines) > 1500 else ''}
            
            CRITICAL: Compare the content against these SPECIFIC brand guidelines.
            Flag inconsistencies with the documented voice, tone, vocabulary, and style rules.
            Reference specific guideline sections in your suggestions.
            """
        elif content_type in ['landing_page', 'product_page', 'other']:
            context_guidance = """
            CONTENT TYPE: Marketing/Landing Page
            
            When providing feedback, focus on:
            - MAJOR issues: broken links, contradictory claims, unprofessional content
            - Do NOT flag normal marketing variation (headlines vs CTAs, legal vs marketing copy)
            - Only flag EXTREME voice inconsistencies (professional → unprofessional)
            
            IMPORTANT - Product Listings:
            If the content appears to be a product listing or grid (repeated product names, 
            prices, "Shop now" buttons, or similar structured e-commerce content), this is 
            intentional formatting, NOT incoherent text. The text extraction may have lost 
            the visual layout, making it appear jumbled. Do NOT flag product grids as 
            coherence issues unless there are actual contradictions or errors in the product 
            information itself.
            """
        elif content_type in ['blog', 'article', 'news']:
            context_guidance = """
            CONTENT TYPE: Editorial/Blog/News
            
            When providing feedback, apply strict editorial standards:
            - Brand voice consistency throughout
            - No broken links or contradictions
            - High professional quality expected
            """
        else:
            context_guidance = """
            CONTENT TYPE: General/Social
            
            Apply standard coherence criteria when providing feedback.
            """
        
        # Step 1: Simple scoring prompt
        score_prompt = f"""
        Score the COHERENCE of this content on a scale of 0.0 to 1.0.
        
        Coherence evaluates: consistency with brand messaging, logical flow, professional quality
        
        Content:
        Title: {content.title}
        Body: {content.body[:2000]}
        Source: {content.src}
        
        Brand Context: {brand_context.get('keywords', [])}
        
        Scoring criteria:
        - 0.8-1.0: Highly coherent, consistent with brand, professional quality
        - 0.6-0.8: Mostly coherent, good consistency
        - 0.4-0.6: Some coherence, minor inconsistencies
        - 0.2-0.4: Limited coherence, noticeable inconsistencies
        - 0.0-0.2: Incoherent, inconsistent, unprofessional
        
        Return only a number between 0.0 and 1.0:
        """
        
        # Use two-step scoring with feedback
        result = self._get_llm_score_with_feedback(
            score_prompt=score_prompt,
            content=content,
            dimension="Coherence",
            context_guidance=context_guidance
        )
        
        # Filter issues based on our strict criteria
        issues = result.get('issues', [])
        filtered_issues = []
        for issue in issues:
            confidence = issue.get('confidence', 0.0)
            if confidence >= 0.7:
                # Special validation for inconsistent_voice
                if issue.get('type') == 'inconsistent_voice':
                    evidence = issue.get('evidence', '').lower()
                    
                    # Reject if evidence is footer/boilerplate text
                    footer_indicators = [
                        '©', 'copyright', 'all rights reserved',
                        'privacy policy', 'terms of use', 'contact us',
                        'grievance redressal', 'global privacy'
                    ]
                    
                    # Reject if evidence doesn't show CONTRAST
                    if 'contrast:' not in evidence and 'vs' not in evidence:
                        logger.debug(f"Filtered inconsistent_voice: no contrast shown in evidence")
                        continue
                    
                    # Reject if evidence contains footer indicators
                    if any(indicator in evidence for indicator in footer_indicators):
                        logger.debug(f"Filtered inconsistent_voice: footer text detected")
                        continue
                    
                    # Reject if evidence is just repeated text (same phrase appears twice)
                    import re
                    quotes = re.findall(r"'([^']+)'", evidence)
                    if len(quotes) >= 2:
                        if quotes[0].lower().strip() == quotes[1].lower().strip():
                            logger.debug(f"Filtered inconsistent_voice: repeated text detected")
                            continue
                
                filtered_issues.append(issue)
            else:
                logger.debug(f"Filtered low-confidence Coherence issue: {issue.get('type')} (confidence={confidence})")
        
        # Store LLM-identified issues in content metadata for later merging
        if not hasattr(content, '_llm_issues'):
            content._llm_issues = {}
        content._llm_issues['coherence'] = filtered_issues
        
        base_score = result.get('score', 0.5)
        
        # Apply content-type multiplier from rubric configuration
        multiplier = self._get_score_multiplier('coherence', content_type)
        
        if multiplier != 1.0:
            adjusted_score = min(1.0, base_score * multiplier)
            logger.info(f"Coherence scoring for {content.content_id[:20]}...")
            logger.info(f"  Content type: {content_type}")
            logger.info(f"  Base LLM score: {base_score:.3f} ({base_score*100:.1f}%)")
            logger.info(f"  Multiplier applied: {multiplier:.2f}x")
            logger.info(f"  Adjusted score: {adjusted_score:.3f} ({adjusted_score*100:.1f}%)")
            return adjusted_score
        
        logger.debug(f"Coherence score for {content_type}: {base_score:.3f} (no multiplier)")
        return base_score
    
    def _determine_content_type(self, content: NormalizedContent) -> str:
        """
        Determine content type based on URL patterns and metadata.
        Simplified version for scorer (full version is in attribute_detector)
        """
        url_lower = content.url.lower() if hasattr(content, 'url') and content.url else ""
        
        # Check for blog/article/news patterns
        if any(p in url_lower for p in ['/blog/', '/article/', '/news/', '/story/']):
            if '/blog/' in url_lower:
                return 'blog'
            elif '/news/' in url_lower or '/story/' in url_lower:
                return 'news'
            else:
                return 'article'
        
        # Check for legal/policy pages (terms, privacy, legal disclaimers)
        legal_patterns = [
            '/terms', '/privacy', '/legal/', '/policy', '/policies/',
            '/conditions', '/disclaimer', '/compliance', '/gdpr',
            'terms-of-use', 'terms-and-conditions', 'privacy-policy'
        ]
        if any(p in url_lower for p in legal_patterns):
            return 'legal_policy'
        
        # Check for landing page patterns
        if (url_lower.endswith('/') or '/product/' in url_lower or 
            '/solution/' in url_lower or '/about' in url_lower or '/home' in url_lower):
            return 'landing_page'
        
        # Check channel
        if hasattr(content, 'channel'):
            if content.channel in ['reddit', 'twitter', 'facebook', 'instagram']:
                return 'social_post'
        
        return 'other'

    
    def _get_score_multiplier(self, dimension: str, content_type: str) -> float:
        """
        Get score multiplier for a dimension and content type from rubric configuration
        
        Args:
            dimension: Dimension name (coherence, verification, etc.)
            content_type: Content type (landing_page, blog, etc.)
        
        Returns:
            Multiplier value (default 1.0 if not configured)
        """
        try:
            from scoring.rubric import load_rubric
            rubric = load_rubric()
            
            multipliers = rubric.get('score_multipliers', {})
            dimension_multipliers = multipliers.get(dimension, {})
            
            # Try to get content-type-specific multiplier
            multiplier = dimension_multipliers.get(content_type)
            
            # Fall back to _default if not found
            if multiplier is None:
                multiplier = dimension_multipliers.get('_default', 1.0)
            
            # Ensure it's a valid number
            return float(multiplier) if multiplier is not None else 1.0
            
        except Exception as e:
            logger.warning(f"Failed to load score multiplier for {dimension}/{content_type}: {e}")
            return 1.0
    
    def _load_brand_guidelines(self, brand_id: str) -> Optional[str]:
        """
        Load brand guidelines from storage if available.
        
        Args:
            brand_id: Brand identifier
        
        Returns:
            Guidelines text or None if not found
        """
        if not brand_id:
            return None
        
        try:
            from utils.document_processor import BrandGuidelinesProcessor
            processor = BrandGuidelinesProcessor()
            guidelines = processor.load_guidelines(brand_id)
            if guidelines:
                logger.info(f"Loaded brand guidelines for {brand_id}: {len(guidelines)} characters")
            return guidelines
        except Exception as e:
            logger.warning(f"Failed to load brand guidelines for {brand_id}: {e}")
            return None
    
    def _score_resonance(self, content: NormalizedContent, brand_context: Dict[str, Any]) -> float:
        """Score Resonance dimension: cultural fit, organic engagement"""
        
        # Use engagement metrics for resonance scoring
        engagement_score = self._calculate_engagement_resonance(content)
        
        prompt = f"""
        Score the RESONANCE of this content on a scale of 0.0 to 1.0.
        
        Resonance evaluates: cultural fit, authentic engagement, organic appeal
        
        Content:
        Title: {content.title}
        Body: {content.body}
        
        Engagement Metrics:
        Rating: {content.rating}
        Upvotes: {content.upvotes}
        Helpful Count: {content.helpful_count}
        
        Brand Context: {brand_context.get('keywords', [])}
        
        Scoring criteria:
        - 0.8-1.0: High cultural fit, strong organic engagement
        - 0.6-0.8: Good resonance, positive engagement
        - 0.4-0.6: Moderate resonance, mixed engagement
        - 0.2-0.4: Low resonance, limited engagement
        - 0.0-0.2: No resonance, negative or artificial engagement
        
        Return only a number between 0.0 and 1.0:
        """
        
        llm_score = self._get_llm_score(prompt)
        
        # Combine LLM score with engagement metrics (70% LLM, 30% engagement)
        combined_score = (0.7 * llm_score) + (0.3 * engagement_score)
        
        return min(1.0, max(0.0, combined_score))
    
    def _calculate_engagement_resonance(self, content: NormalizedContent) -> float:
        """Calculate engagement-based resonance score"""
        score = 0.5  # Default neutral score

        # Rating-based scoring (0-1 scale)
        if content.rating is not None:
            if content.src == "amazon":
                # Amazon ratings are 1-5, convert to 0-1
                score += (content.rating - 3) * 0.1
            elif content.src == "reddit":
                # Reddit upvote ratio is 0-1, use directly
                score += (content.rating - 0.5) * 0.2

        # Upvotes-based scoring
        if content.upvotes is not None:
            # Normalize upvotes (log scale to prevent outliers from dominating)
            import math
            normalized_upvotes = math.log10(max(1, content.upvotes)) / 3  # Log base 10, max ~3
            score += normalized_upvotes * 0.1

        # Helpful count scoring (Amazon reviews)
        if content.helpful_count is not None:
            normalized_helpful = min(content.helpful_count / 20, 1.0)  # Cap at 20 helpful votes
            score += normalized_helpful * 0.1

        return min(1.0, max(0.0, score))

    def _adjust_scores_with_attributes(self, llm_scores: DimensionScores,
                                      detected_attrs: List[DetectedAttribute]) -> DimensionScores:
        """
        Adjust LLM-based dimension scores using detected Trust Stack attributes

        Args:
            llm_scores: Base scores from LLM (0.0-1.0 scale)
            detected_attrs: List of detected attributes from TrustStackAttributeDetector

        Returns:
            Adjusted dimension scores (0.0-1.0 scale)
        """
        # Start with LLM scores (convert to 0-100 for adjustment calculation)
        adjusted = {
            'provenance': llm_scores.provenance * 100,
            'resonance': llm_scores.resonance * 100,
            'coherence': llm_scores.coherence * 100,
            'transparency': llm_scores.transparency * 100,
            'verification': llm_scores.verification * 100
        }

        # Group attributes by dimension
        attrs_by_dimension = {}
        for attr in detected_attrs:
            if attr.dimension not in attrs_by_dimension:
                attrs_by_dimension[attr.dimension] = []
            attrs_by_dimension[attr.dimension].append(attr)

        # Adjust each dimension based on its detected attributes
        for dimension, attrs in attrs_by_dimension.items():
            if dimension not in adjusted:
                continue

            # Calculate adjustment from attributes (1-10 scale → adjustment)
            # Strategy: Blend attribute signals with LLM baseline
            # Attributes with high confidence and extreme values have more impact
            total_adjustment = 0.0
            total_weight = 0.0

            for attr in attrs:
                # Skip negative adjustments for LLM-identified issues
                # The LLM score already accounts for these issues, so applying
                # negative adjustments would double-penalize
                is_llm_only = attr.evidence and attr.evidence.startswith("LLM:")
                
                # Map 1-10 scale to adjustment (-50 to +50)
                # 1 = -45, 5.5 = 0, 10 = +45
                attr_adjustment = (attr.value - 5.5) * 9

                # Skip negative adjustments for LLM-only attributes
                if is_llm_only and attr_adjustment < 0:
                    logger.debug(f"Skipping negative adjustment for LLM-only attribute {attr.label} in {dimension}")
                    continue

                # Weight by confidence
                weight = attr.confidence
                total_adjustment += attr_adjustment * weight
                total_weight += weight

            if total_weight > 0:
                # Average weighted adjustment
                avg_adjustment = total_adjustment / total_weight

                # Apply adjustment with dampening (70% LLM, 30% attributes)
                # This ensures LLM baseline is respected while attributes provide nuance
                adjusted[dimension] = (
                    0.7 * adjusted[dimension] +
                    0.3 * max(0, min(100, adjusted[dimension] + avg_adjustment))
                )

                # Clamp to valid range
                adjusted[dimension] = max(0, min(100, adjusted[dimension]))

        # Convert back to 0.0-1.0 scale
        return DimensionScores(
            provenance=adjusted['provenance'] / 100,
            resonance=adjusted['resonance'] / 100,
            coherence=adjusted['coherence'] / 100,
            transparency=adjusted['transparency'] / 100,
            verification=adjusted['verification'] / 100
        )
    
    def _get_llm_score(self, prompt: str) -> float:
        """Get score from LLM API (delegates to LLMScoringClient)"""
        return self.llm_client.get_score(prompt)
    
    def _get_llm_score_with_reasoning(self, prompt: str) -> Dict[str, Any]:
        """
        Get score AND reasoning from LLM API (delegates to LLMScoringClient)
        
        Returns:
            Dictionary with 'score' (float) and 'issues' (list of dicts)
        """
        return self.llm_client.get_score_with_reasoning(prompt)
    
    def _get_llm_score_with_feedback(self, score_prompt: str, content: NormalizedContent, 
                                     dimension: str, context_guidance: str = "") -> Dict[str, Any]:
        """
        Two-step LLM scoring: Get score first, then get feedback (delegates to LLMScoringClient)
        
        Args:
            score_prompt: Prompt to get the score (0.0-1.0)
            content: Content being scored
            dimension: Dimension name (for logging)
            context_guidance: Optional context about content type
        
        Returns:
            Dictionary with 'score' (float) and 'issues' (list of dicts)
        """
        return self.llm_client.get_score_with_feedback(score_prompt, content, dimension, context_guidance)
    
    def _merge_llm_and_detector_issues(self, content: NormalizedContent, 
                                      detected_attrs: List[DetectedAttribute]) -> List[DetectedAttribute]:
        """
        Merge LLM-identified issues with detector-found attributes
        
        Args:
            content: Content object (may have _llm_issues attribute)
            detected_attrs: List of attributes detected by attribute detector
        
        Returns:
            Merged list of DetectedAttribute objects with source tracking
        """
        from scoring.issue_mapper import map_llm_issue_to_attribute
        from scoring.link_verifier import verify_broken_links
        
        merged_attrs = []
        
        # Track which attributes we've seen from the detector
        detector_attr_ids = {attr.attribute_id for attr in detected_attrs}
        
        # Add all detector-found attributes (mark as detector-only or both)
        for attr in detected_attrs:
            merged_attrs.append(attr)
        
        # Process LLM issues if they exist
        if hasattr(content, '_llm_issues'):
            # DIAGNOSTIC: Log LLM issues by dimension
            for dim, issues in content._llm_issues.items():
                if issues:
                    logger.info(f"[MERGE DIAGNOSTIC] Processing {len(issues)} LLM issues for {dim} dimension")
            
            for dimension, llm_issues in content._llm_issues.items():
                for llm_issue in llm_issues:
                    issue_type = llm_issue.get('type', '')
                    
                    # Special handling for broken_links: verify with actual HTTP checks
                    if issue_type in ['broken_links', 'outdated_links']:
                        content_text = f"{content.title} {content.body}"
                        content_url = getattr(content, 'url', None)
                        actual_broken_links = verify_broken_links(content_text, content_url)
                        
                        if not actual_broken_links:
                            # LLM hallucinated broken links - reject this issue
                            logger.warning(f"LLM hallucinated broken_links for content {content.content_id} - no actual broken links found")
                            continue
                        else:
                            # Update evidence with actual broken link URLs
                            broken_urls = [link['url'] for link in actual_broken_links[:3]]  # Max 3 examples
                            llm_issue['evidence'] = f"Verified broken links: {', '.join(broken_urls)}"
                            logger.info(f"Verified {len(actual_broken_links)} broken links for content {content.content_id}")
                    
                    # Map LLM issue type to attribute ID
                    attr_id = map_llm_issue_to_attribute(issue_type)
                    
                    # DIAGNOSTIC: Log mapping result
                    logger.info(f"[MERGE DIAGNOSTIC] Mapping '{issue_type}' ({dimension}) → attribute_id: {attr_id}")
                    
                    if not attr_id:
                        # Log unmapped issues for debugging
                        logger.warning(f"[MERGE DIAGNOSTIC] UNMAPPED LLM issue type '{issue_type}' in {dimension} dimension for content {content.content_id}")
                        continue
                    
                    # Check if detector also found this issue
                    if attr_id in detector_attr_ids:
                        # Both found it - increase confidence of existing attribute
                        for attr in merged_attrs:
                            if attr.attribute_id == attr_id:
                                # Boost confidence when both LLM and detector agree
                                attr.confidence = min(1.0, attr.confidence * 1.2)
                                # Enhance evidence with LLM reasoning
                                llm_evidence = llm_issue.get('evidence', '')
                                if llm_evidence and llm_evidence not in attr.evidence:
                                    attr.evidence = f"{attr.evidence} | LLM: {llm_evidence}"
                                break
                    else:
                        # Only LLM found it - create new attribute
                        # Get label from rubric or use issue type
                        label = llm_issue.get('type', '').replace('_', ' ').title()
                        
                        # Determine value based on severity
                        severity = llm_issue.get('severity', 'medium')
                        if severity == 'high':
                            value = 2.0
                        elif severity == 'medium':
                            value = 5.0
                        else:  # low
                            value = 7.0
                        
                        # Extract suggestion from LLM issue
                        suggestion = llm_issue.get('suggestion', None)
                        
                        new_attr = DetectedAttribute(
                            attribute_id=attr_id,
                            dimension=dimension,
                            label=label,
                            value=value,
                            evidence=f"LLM: {llm_issue.get('evidence', 'Issue detected')}",
                            confidence=0.6,  # Lowered from 0.7 for more LLM-only attributes
                            suggestion=suggestion  # Preserve LLM suggestion
                        )
                        merged_attrs.append(new_attr)
                        detector_attr_ids.add(attr_id)
                        logger.info(f"[MERGE DIAGNOSTIC] Added LLM-only attribute '{label}' (value={value}) for {dimension} dimension")
        
        # DIAGNOSTIC: Log final merged attributes by dimension
        attrs_by_dim = {}
        for attr in merged_attrs:
            attrs_by_dim[attr.dimension] = attrs_by_dim.get(attr.dimension, 0) + 1
        logger.info(f"[MERGE DIAGNOSTIC] Final merged attributes by dimension: {attrs_by_dim}")
        
        return merged_attrs
    
    def batch_score_content(self, content_list: List[NormalizedContent],
                          brand_context: Dict[str, Any]) -> List[ContentScores]:
        """
        Score multiple content items in batch
        Combines LLM scoring with Trust Stack attribute detection

        Args:
            content_list: List of content to score
            brand_context: Brand-specific context

        Returns:
            List of ContentScores with dimension ratings
        """
        from scoring.content_filter import should_skip_content
        
        scores_list = []

        logger.info(f"Batch scoring {len(content_list)} content items (attribute detection: {self.use_attribute_detection})")

        for i, content in enumerate(content_list):
            if i % 10 == 0:
                logger.info(f"Scoring progress: {i}/{len(content_list)}")

            # Pre-filter: Skip error pages, login walls, and insufficient content
            skip_reason = should_skip_content(
                title=getattr(content, 'title', ''),
                body=getattr(content, 'body', ''),
                url=getattr(content, 'url', '')
            )
            
            if skip_reason:
                logger.warning(f"Skipping content '{content.title}' ({content.content_id}): {skip_reason}")
                # Don't add to scores_list - effectively filters it out
                continue

            # Step 1: Get base LLM scores
            dimension_scores = self.score_content(content, brand_context)

            # Step 2: Detect Trust Stack attributes (if enabled)
            detected_attrs = []
            if self.use_attribute_detection and self.attribute_detector:
                try:
                    detected_attrs = self.attribute_detector.detect_attributes(content)
                    logger.debug(f"Detected {len(detected_attrs)} attributes for {content.content_id}")

                    # Step 2.5: Merge LLM issues with detector attributes
                    detected_attrs = self._merge_llm_and_detector_issues(content, detected_attrs)
                    logger.debug(f"After merging: {len(detected_attrs)} total attributes")

                    # Adjust LLM scores with attribute signals
                    dimension_scores = self._adjust_scores_with_attributes(dimension_scores, detected_attrs)
                except Exception as e:
                    logger.warning(f"Attribute detection failed for {content.content_id}: {e}")

            # Step 3: Create ContentScores object
            content_scores = ContentScores(
                content_id=content.content_id,
                brand=brand_context.get('brand_name', 'unknown'),
                src=content.src,
                event_ts=content.event_ts,
                score_provenance=dimension_scores.provenance,
                score_resonance=dimension_scores.resonance,
                score_coherence=dimension_scores.coherence,
                score_transparency=dimension_scores.transparency,
                score_verification=dimension_scores.verification,
                class_label="",  # Optional - for backward compatibility
                is_authentic=False,  # Optional - for backward compatibility
                rubric_version=self.rubric_version,
                run_id=content.run_id,
                # Enhanced Trust Stack fields
                modality=getattr(content, 'modality', 'text'),
                channel=getattr(content, 'channel', 'unknown'),
                platform_type=getattr(content, 'platform_type', 'unknown'),
                meta=json.dumps(
                    # Build a meta dict that includes scoring info and detected attributes
                    (lambda cm: {
                        "scoring_timestamp": content.event_ts,
                        "brand_context": brand_context,
                        "title": getattr(content, 'title', '') or None,
                        "description": getattr(content, 'body', '') or None,
                        "source_url": (cm.get('source_url') if isinstance(cm, dict) else None) or getattr(content, 'platform_id', None),
                        # Enhanced Trust Stack metadata
                        "modality": getattr(content, 'modality', 'text'),
                        "channel": getattr(content, 'channel', 'unknown'),
                        "platform_type": getattr(content, 'platform_type', 'unknown'),
                        "url": getattr(content, 'url', ''),
                        "language": getattr(content, 'language', 'en'),
                        # Include detected attributes for downstream analysis
                        "detected_attributes": [
                            {
                                "id": attr.attribute_id,
                                "dimension": attr.dimension,
                                "label": attr.label,
                                "value": attr.value,
                                "evidence": attr.evidence,
                                "confidence": attr.confidence,
                                "suggestion": attr.suggestion  # Include LLM suggestion
                            }
                            for attr in detected_attrs
                        ] if detected_attrs else [],
                        "attribute_count": len(detected_attrs),
                        # preserve any existing content.meta under orig_meta
                        "orig_meta": cm if isinstance(cm, dict) else None,
                        # propagate explicit footer links if present so downstream reporting can use them
                        **({
                            'terms': cm.get('terms'),
                            'privacy': cm.get('privacy')
                        } if isinstance(cm, dict) and (cm.get('terms') or cm.get('privacy')) else {})
                    })(content.meta if hasattr(content, 'meta') else {})
                )
            )

            scores_list.append(content_scores)

        logger.info(f"Completed batch scoring: {len(scores_list)} items scored")
        return scores_list
