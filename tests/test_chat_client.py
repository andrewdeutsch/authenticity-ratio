"""
Tests for OpenAI ChatClient and provenance helper.

These tests verify:
- ChatClient initialization and configuration
- Chat completion API calls (mocked)
- Provenance labeling helper function
- Error handling and fallbacks
"""

import os
import pytest
from unittest.mock import Mock, patch, MagicMock


def test_add_llm_provenance():
    """Test the centralized provenance labeling helper."""
    from reporting.markdown_generator import add_llm_provenance

    # Basic usage
    result = add_llm_provenance("This is a summary.", "gpt-3.5-turbo")
    assert result == "This is a summary. (Generated by gpt-3.5-turbo)"

    # Different model
    result = add_llm_provenance("Another summary.", "gpt-4")
    assert result == "Another summary. (Generated by gpt-4)"

    # Empty text handling
    result = add_llm_provenance("", "gpt-3.5-turbo")
    assert result == ""

    result = add_llm_provenance(None, "gpt-3.5-turbo")
    assert result is None

    # Already has provenance (avoid double-labeling)
    result = add_llm_provenance("Summary. (Generated by gpt-3.5-turbo)", "gpt-3.5-turbo")
    assert result == "Summary. (Generated by gpt-3.5-turbo)"
    assert result.count("Generated by") == 1


def test_chat_client_imports():
    """Test that ChatClient can be imported when openai is available."""
    try:
        from scoring.llm_client import ChatClient
        assert ChatClient is not None
    except ImportError as e:
        # If openai package is not installed, that's expected
        if "openai" in str(e).lower():
            pytest.skip("OpenAI package not installed")
        else:
            raise


@patch.dict(os.environ, {'OPENAI_API_KEY': 'test-key-12345'})
def test_chat_client_initialization():
    """Test ChatClient initialization with mocked OpenAI."""
    with patch('scoring.llm_client.OpenAI') as mock_openai:
        from scoring.llm_client import ChatClient

        # Initialize with API key from environment
        client = ChatClient()

        # Verify OpenAI was called with the API key
        mock_openai.assert_called_once_with(api_key='test-key-12345')
        assert client.default_model == 'gpt-3.5-turbo'


@patch.dict(os.environ, {'OPENAI_API_KEY': 'test-key-12345'})
def test_chat_client_chat_completion():
    """Test ChatClient.chat() method with mocked OpenAI response."""
    with patch('scoring.llm_client.OpenAI') as mock_openai:
        from scoring.llm_client import ChatClient

        # Mock the OpenAI response structure
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message.content = "This is a test summary."
        mock_response.usage = Mock(
            prompt_tokens=50,
            completion_tokens=10,
            total_tokens=60
        )

        mock_client_instance = Mock()
        mock_client_instance.chat.completions.create.return_value = mock_response
        mock_openai.return_value = mock_client_instance

        # Create ChatClient and call chat
        client = ChatClient()
        response = client.chat(
            messages=[{"role": "user", "content": "Summarize this text."}],
            model="gpt-3.5-turbo",
            max_tokens=150
        )

        # Verify response format
        assert response['content'] == "This is a test summary."
        assert response['text'] == "This is a test summary."
        assert response['model'] == "gpt-3.5-turbo"
        assert response['usage']['total_tokens'] == 60

        # Verify the API was called with correct parameters
        mock_client_instance.chat.completions.create.assert_called_once_with(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Summarize this text."}],
            max_tokens=150,
            temperature=0.3
        )


@patch.dict(os.environ, {'OPENAI_API_KEY': 'test-key-12345'})
def test_chat_client_summarize():
    """Test ChatClient.summarize() convenience method."""
    with patch('scoring.llm_client.OpenAI') as mock_openai:
        from scoring.llm_client import ChatClient

        # Mock the OpenAI response
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message.content = "Brief summary of the text."
        mock_response.usage = Mock(prompt_tokens=100, completion_tokens=20, total_tokens=120)

        mock_client_instance = Mock()
        mock_client_instance.chat.completions.create.return_value = mock_response
        mock_openai.return_value = mock_client_instance

        # Create ChatClient and summarize
        client = ChatClient()
        summary = client.summarize("Long text to summarize...", max_words=50)

        assert summary == "Brief summary of the text."

        # Verify the chat completion was called
        assert mock_client_instance.chat.completions.create.called


@patch.dict(os.environ, {}, clear=True)
def test_chat_client_no_api_key():
    """Test ChatClient raises error when no API key is configured."""
    # Defensive: ensure OPENAI_API_KEY is not present in the environment
    os.environ.pop('OPENAI_API_KEY', None)
    with patch('scoring.llm_client.OpenAI'):
        from scoring.llm_client import ChatClient

        # Should raise ValueError when no API key is available
        with pytest.raises(ValueError, match="OpenAI API key not configured"):
            ChatClient()


@patch.dict(os.environ, {'OPENAI_API_KEY': 'test-key-12345'})
def test_chat_client_error_handling():
    """Test ChatClient error handling when API call fails."""
    with patch('scoring.llm_client.OpenAI') as mock_openai:
        from scoring.llm_client import ChatClient, OpenAIError

        # Mock an API error
        mock_client_instance = Mock()
        mock_client_instance.chat.completions.create.side_effect = OpenAIError("API rate limit exceeded")
        mock_openai.return_value = mock_client_instance

        client = ChatClient()

        # Should raise the OpenAI error
        with pytest.raises(OpenAIError):
            client.chat(messages=[{"role": "user", "content": "Test"}])


def test_provenance_in_markdown_generator():
    """Test that provenance helper is used in markdown generator."""
    from reporting.markdown_generator import _llm_summarize

    # This function uses ChatClient which may not be available in test env
    # Just verify it exists and can be called (will return None without API key)
    result = _llm_summarize("Test text", model="gpt-3.5-turbo", max_words=50)

    # Without API key, should return None gracefully
    assert result is None


def test_provenance_in_pdf_generator():
    """Test that provenance helper is imported by PDF generator."""
    try:
        from reporting.pdf_generator import add_llm_provenance
        assert add_llm_provenance is not None

        # Test it works the same way
        result = add_llm_provenance("Summary text.", "gpt-4")
        assert result == "Summary text. (Generated by gpt-4)"
    except ImportError:
        # pdf_generator might not import if reportlab is not installed
        pytest.skip("PDF generator dependencies not available")


if __name__ == '__main__':
    pytest.main([__file__, '-v'])
