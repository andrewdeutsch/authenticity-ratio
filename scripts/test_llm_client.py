#!/usr/bin/env python3
"""
Test script for OpenAI ChatClient and LLM provenance functionality.

This script verifies:
1. ChatClient initialization and configuration
2. Basic chat completion
3. Text summarization
4. Provenance labeling
5. Integration with report generation

Usage:
    # Set your OpenAI API key first
    export OPENAI_API_KEY=sk-...

    # Or create a .env file with OPENAI_API_KEY=sk-...

    # Run the test
    python scripts/test_llm_client.py

    # Run with custom text
    python scripts/test_llm_client.py --text "Your custom text to summarize..."
"""

import os
import sys
import argparse
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from scoring.llm_client import ChatClient
from reporting.markdown_generator import add_llm_provenance, _llm_summarize


def test_provenance_helper():
    """Test the centralized provenance labeling helper."""
    print("=" * 70)
    print("TEST 1: Provenance Helper Function")
    print("=" * 70)

    # Test basic usage
    text = "This is a test summary generated by the LLM."
    result = add_llm_provenance(text, "gpt-3.5-turbo")
    print(f"✓ Basic provenance: {result}")
    assert "(Generated by gpt-3.5-turbo)" in result

    # Test duplicate prevention
    result2 = add_llm_provenance(result, "gpt-3.5-turbo")
    assert result2.count("Generated by") == 1
    print(f"✓ Duplicate prevention: {result2}")

    # Test empty text
    result3 = add_llm_provenance("", "gpt-4")
    print(f"✓ Empty text handling: '{result3}'")

    print("\n✅ Provenance helper tests passed!\n")


def test_chat_client_init():
    """Test ChatClient initialization."""
    print("=" * 70)
    print("TEST 2: ChatClient Initialization")
    print("=" * 70)

    api_key = os.getenv('OPENAI_API_KEY')

    if not api_key:
        print("⚠️  No OPENAI_API_KEY found in environment")
        print("   Set it with: export OPENAI_API_KEY=sk-...")
        print("   Or create a .env file with OPENAI_API_KEY=sk-...")
        return False

    try:
        client = ChatClient()
        print(f"✓ ChatClient initialized successfully")
        print(f"✓ Default model: {client.default_model}")
        return True
    except Exception as e:
        print(f"✗ ChatClient initialization failed: {e}")
        return False


def test_chat_completion(client: ChatClient):
    """Test basic chat completion."""
    print("\n" + "=" * 70)
    print("TEST 3: Chat Completion")
    print("=" * 70)

    messages = [
        {"role": "user", "content": "What is the Authenticity Ratio? Answer in one sentence."}
    ]

    try:
        print("Sending chat completion request...")
        response = client.chat(messages=messages, max_tokens=100, temperature=0.3)

        print(f"✓ Response received")
        print(f"✓ Model: {response['model']}")
        print(f"✓ Content: {response['content']}")
        print(f"✓ Tokens used: {response['usage']['total_tokens']}")
        print("\n✅ Chat completion test passed!\n")
        return True
    except Exception as e:
        print(f"✗ Chat completion failed: {e}")
        return False


def test_summarization(client: ChatClient, custom_text: str = None):
    """Test text summarization."""
    print("=" * 70)
    print("TEST 4: Text Summarization")
    print("=" * 70)

    # Default test text
    if custom_text is None:
        custom_text = """
        The Authenticity Ratio (AR) is a key performance indicator that measures
        authentic versus inauthentic brand-linked content across digital channels.
        It provides CMOs and boards with a quantifiable metric for brand health,
        similar to how Net Promoter Score (NPS) measures customer satisfaction.
        The AR tool analyzes content using five trust dimensions: Provenance
        (origin and traceability), Verification (factual accuracy), Transparency
        (clear disclosures), Coherence (consistency with brand messaging), and
        Resonance (cultural fit and engagement). Content is scored on each dimension
        and classified as Authentic, Suspect, or Inauthentic, allowing brands to
        identify and address content quality issues systematically.
        """

    print(f"Original text ({len(custom_text)} chars):")
    print("-" * 70)
    print(custom_text.strip())
    print("-" * 70)

    try:
        print("\nGenerating summary...")
        summary = client.summarize(custom_text, max_words=50)

        if summary:
            print(f"✓ Summary generated ({len(summary)} chars):")
            print("-" * 70)
            print(summary)
            print("-" * 70)

            # Add provenance
            summary_with_provenance = add_llm_provenance(summary, client.default_model)
            print(f"\n✓ With provenance:")
            print(summary_with_provenance)

            print("\n✅ Summarization test passed!\n")
            return True
        else:
            print("✗ No summary generated")
            return False
    except Exception as e:
        print(f"✗ Summarization failed: {e}")
        return False


def test_llm_summarize_integration():
    """Test the _llm_summarize function used by report generators."""
    print("=" * 70)
    print("TEST 5: Report Generator Integration (_llm_summarize)")
    print("=" * 70)

    test_text = "This is a test document about brand authenticity and content verification."

    try:
        print("Testing _llm_summarize function...")
        result = _llm_summarize(test_text, model='gpt-3.5-turbo', max_words=20)

        if result:
            print(f"✓ Summary: {result}")

            # Add provenance using helper
            with_provenance = add_llm_provenance(result, 'gpt-3.5-turbo')
            print(f"✓ With provenance: {with_provenance}")

            print("\n✅ Report integration test passed!\n")
            return True
        else:
            print("⚠️  _llm_summarize returned None (expected if API key not set)")
            return None
    except Exception as e:
        print(f"⚠️  Report integration test skipped: {e}")
        return None


def main():
    parser = argparse.ArgumentParser(description='Test OpenAI ChatClient and LLM provenance')
    parser.add_argument('--text', type=str, help='Custom text to summarize')
    parser.add_argument('--skip-api', action='store_true', help='Skip tests requiring API calls')
    args = parser.parse_args()

    print("\n" + "=" * 70)
    print("🧪 OpenAI ChatClient Test Suite")
    print("=" * 70)
    print()

    # Track results
    results = {}

    # Test 1: Provenance helper (no API required)
    try:
        test_provenance_helper()
        results['provenance'] = True
    except Exception as e:
        print(f"✗ Provenance test failed: {e}\n")
        results['provenance'] = False

    if args.skip_api:
        print("⏭️  Skipping API tests (--skip-api flag set)")
        return

    # Test 2: Initialize client
    client = None
    try:
        init_success = test_chat_client_init()
        results['init'] = init_success

        if init_success:
            client = ChatClient()
    except Exception as e:
        print(f"✗ Initialization test failed: {e}\n")
        results['init'] = False

    if not client:
        print("\n⚠️  Cannot proceed with API tests - ChatClient not initialized")
        print("   Make sure OPENAI_API_KEY is set in your environment or .env file")
        print_summary(results)
        sys.exit(1)

    # Test 3: Chat completion
    try:
        results['chat'] = test_chat_completion(client)
    except Exception as e:
        print(f"✗ Chat test failed: {e}\n")
        results['chat'] = False

    # Test 4: Summarization
    try:
        results['summarize'] = test_summarization(client, args.text)
    except Exception as e:
        print(f"✗ Summarization test failed: {e}\n")
        results['summarize'] = False

    # Test 5: Report integration
    try:
        results['integration'] = test_llm_summarize_integration()
    except Exception as e:
        print(f"✗ Integration test failed: {e}\n")
        results['integration'] = False

    # Summary
    print_summary(results)


def print_summary(results):
    """Print test summary."""
    print("\n" + "=" * 70)
    print("📊 TEST SUMMARY")
    print("=" * 70)

    passed = sum(1 for v in results.values() if v is True)
    failed = sum(1 for v in results.values() if v is False)
    skipped = sum(1 for v in results.values() if v is None)
    total = len(results)

    for test_name, result in results.items():
        status = "✅ PASS" if result is True else ("❌ FAIL" if result is False else "⏭️  SKIP")
        print(f"{status:12} {test_name}")

    print("-" * 70)
    print(f"Total: {total} | Passed: {passed} | Failed: {failed} | Skipped: {skipped}")
    print("=" * 70)

    if failed > 0:
        print("\n❌ Some tests failed. See output above for details.")
        sys.exit(1)
    elif passed > 0:
        print("\n✅ All tests passed!")
    else:
        print("\n⚠️  No tests were run.")


if __name__ == '__main__':
    main()
